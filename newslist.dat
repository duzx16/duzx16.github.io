**[23/01/23]** Our paper [GLM-130B: An Open Bilingual Pre-trained Model](/publications/glm-130b) is accepted at ICLR 2023.
**[04/08/22]** We release [GLM-130B](https://github.com/THUDM/GLM-130B), an open bilingual (English & Chinese) bidirectional dense model with 130 billion parameters, pre-trained using the [General Language Model (GLM)](/publications/glm-21) algorithm.
**[24/02/22]** Our paper [GLM: General Language Model Pretraining with Autoregressive Blank Infilling](/publications/glm-21) is accepted at ACL 2022.