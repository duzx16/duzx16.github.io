[{"authors":null,"categories":null,"content":"I am a PhD student in the Department of Computer Science and Technology at Tsinghua University. I am also a member of the Knowledge Engineering Group, advised by Prof. Jie Tang.\nMy research interest lies in the application of machine learning algorithms to real-world systems, including information retrieval [ECML/PKDD'18,KDD'19,TKDE,SIGIR'19], knowledge graphs [TKDE], and pretrained language models[arxiv,arxiv]. Generally, I am also interested in the algorithms for machine learning and reinforcement learning.\nIn my spare time, I like to read science fiction novels and history books, especially the history of Ancient Rome and Three Kingdoms Period in China.\n","date":1634256000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1634256000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zxdu.xyz/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student in the Department of Computer Science and Technology at Tsinghua University. I am also a member of the Knowledge Engineering Group, advised by Prof. Jie Tang.\nMy research interest lies in the application of machine learning algorithms to real-world systems, including information retrieval [ECML/PKDD'18,KDD'19,TKDE,SIGIR'19], knowledge graphs [TKDE], and pretrained language models[arxiv,arxiv]. Generally, I am also interested in the algorithms for machine learning and reinforcement learning.","tags":null,"title":"Zhengxiao Du","type":"authors"},{"authors":["Zhengxiao Du","Chang Zhou","Jiangchao Yao","Ming Ding","Hongxia Yang","Jie Tang"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"06040017fb42f695dbd287fb1ba81fa8","permalink":"https://zxdu.xyz/publication/cogkr-19/","publishdate":"2019-10-15T13:59:10.99865Z","relpermalink":"/publication/cogkr-19/","section":"publication","summary":"Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-art approaches.","tags":null,"title":"CogKR: Cognitive Graph for Multi-hop Knowledge Reasoning","type":"publication"},{"authors":["Zhengxiao Du","Yujie Qian","Xiao Liu","Ming Ding","Jiezhong Qiu","Zhilin Yang","Jie Tang"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"c5af62dffbcd378227f2f3810518155b","permalink":"https://zxdu.xyz/publication/glm-21/","publishdate":"2019-10-15T13:59:10.99865Z","relpermalink":"/publication/glm-21/","section":"publication","summary":"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks differ in nature, with three main categories being natural language understanding (NLU), unconditional generation, and conditional generation, while none of the pretraining frameworks performs the best for all tasks. We propose a General Language Model (GLM)  based on autoregressive blank infilling to address this challenge. The proposed architecture has two major benefits: (1) It improves pretrain-finetune consistency via cloze-style finetuning and naturally handles variable-length blank infilling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pretraining data and steps. (2) It is flexible enough to handle various NLP tasks with a single pretrained model. GLM with 1.25x parameters of BERT-Large achieves the best performance in NLU, conditional and unconditional generation at the same time, demonstrating its generalizability to different downstream tasks.","tags":null,"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling","type":"publication"},{"authors":["Xiao Liu","Yanan Zheng","Zhengxiao Du","Ming Ding","Jiezhong Qiu","Zhilin Yang","Jie Tang"],"categories":null,"content":"","date":1634256000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1634256000,"objectID":"3f1a92fc0bd524a9c799eb0c0c74a76d","permalink":"https://zxdu.xyz/publication/p-tuning/","publishdate":"2019-10-15T13:59:10.99865Z","relpermalink":"/publication/p-tuning/","section":"publication","summary":"While GPTs with traditional ﬁne-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuningwhich employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we ﬁnd that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.","tags":null,"title":"GPT Understands, Too","type":"publication"},{"authors":["Himank Yadav","Zhengxiao Du","Thorsten Joachims"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"91e7107418f0ee03339fca8b6d069ba8","permalink":"https://zxdu.xyz/publication/fair-19/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/publication/fair-19/","section":"publication","summary":"While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and attractive source of data for learning to rank, it can produce unfair ranking policies for both exogenous and endogenous reasons. Exogenous reasons typically manifest themselves as biases in the training data, which then get reflected in the learned ranking policy and often lead to rich-get-richer dynamics. Moreover, even after the correction of such biases, reasons endogenous to the design of the learning algorithm can still lead to ranking policies that do not allocate exposure among items in a fair way. To address both exogenous and endogenous sources of unfairness, we present the first learning-to-rank approach that addresses both presentation bias and merit-based fairness of exposure simultaneously. Specifically, we define a class of amortized fairness-of-exposure constraints that can be chosen based on the needs of an application, and we show how these fairness criteria can be enforced despite the selection biases in implicit feedback data. The key result is an efficient and flexible policy-gradient algorithm, called FULTR, which is the first to enable the use of counterfactual estimators for both utility estimation and fairness constraints. Beyond the theoretical justification of the framework, we show empirically that the proposed algorithm can learn accurate and fair ranking policies from biased and noisy feedback. ","tags":null,"title":"Policy-Gradient Training of Fair and Unbiased Ranking Functions","type":"publication"},{"authors":["Zhengxiao Du"],"categories":["阅读"],"content":"为了记录自己最近看的书决定开两个专栏，一个是读书笔记，用来记录偏技术性的读书总结，一个是读书感想，用来发表非技术性的读书体会。本篇比较接近一篇安利贴。\n这是一本《哈利波特》的同人小说。《哈利波特》的原著是我阅读次数最多的小说之一，当然都是在初中时期看的。我觉得这也是比较容易欣赏这本同人的要求：对原著小说有一定的了解，但是又没有热爱到不允许出现任何和原著不符的情节和人物。\n当下大部分的《哈利波特》同人无外乎是两种类型，一种是把自己当成原著剧情的补充和扩展，专注于原著主线之外的情节和人物关系；另一种则是只保留原著的世界观，但是讲述完全不同的故事，甚至连主人公都变了（《舌尖上的霍格沃滋》）。而本书则恰好介于两者之间：本书的剧情完全基于原著第一部《魔法石》，没有引入任何新的人物，但是整个情节发展却完全不同。不同在于，如果说原著小说是给儿童的童话故事，而本书则是给成年人的奇幻小说。\n用理性精神探索魔法世界 本书的世界线与原著的最明显不同在于，由于某些原因，哈利的姨妈并没有嫁给德思礼，而是嫁给了牛津大学的物理学教授万瑞斯。哈利从小就收到了万瑞斯教授的呵护和教育，收到了良好的理性精神和科学方法的训练。而在他十一岁的时候，他像原著一样收到了猫头鹰寄来的霍格沃滋的录取通知书。就这样，一个有着科学家头脑的男孩走入了充满不合理的魔法世界。\n因为这种科学世界观被魔法世界冲击的感受，本书带上了浓厚的科幻色彩。当哈利见到麦格教授在他面前变成一只猫时，他因为能量守恒被打破和猫的大脑居然能承载人的思想而跳了起来。值得注意的是，本书并没有强行用现有的科学知识去解释魔法，而是很快承认了魔法的存在打破了现有的科学定理。但是，虽然已有的知识被打破了，但是发现这些知识的方法却没有失效。哈利的人生理想很快变成了用科学的方法论来探索魔法的原理，从而实现自己改造世界的目标。\n在哈利不断接触魔法世界的过程中，作者也为原著中稍显不合理的设定增加了更为自洽和合理的解释。这一类情节的高峰就是时间转换器的相关情节。对原著比较了解的人都知道，时间转换器是原文中非常严重的一个bug。它的能力过于强大，以至于罗琳也不得不在第五部将它们全部毁掉。时间转换器涉及的是科幻小说中一个经典的主题：回到过去。这里作者采用的是“时间线唯一且过去不可改变”的设定，即不存在平行宇宙，并且回到过去无法改变过去。未来的人回到过去的所作所为构成了过去的一部分。如哈利所说，这意味着整个世界是一个超图灵机，因为当下的计算可以利用来自未来的信息。这一设定可谓科幻味十足，并且充满了可利用的空间。对于未来的自己来说，已经知道的事实是无法改变的，但是还不知道的事实却可以回到过去加以影响。因此只要自己不知道某个人是否已经知道了某条信息，就可以利用时间转换器回到过去，将这条信息告诉他/她。\n对于受过科学训练的人来说，另一个剧情上的吸引力在于用科学方法去探索和解释未知世界。这一类情节的高峰同样是关于时间转换器。当得知关于时间转换器的上述设定之后，哈利立刻意识到，这种超图灵机结构完全可以被用来构建解决NP问题的高效算法。于是他进行了如下实验：请一个朋友随机选择100-999之间的两个质数，将两者的乘积告诉自己。然后自己回到房间，捡起未来的自己写给自己的纸条（称为纸1）。然后从笔记本上撕下一张纸（称为纸2）。根据纸1的内容在纸2上书写。如果纸1为空，则在纸一上写下“100 100”。否则将纸1上的两个数字相乘，如果乘积与之前的乘积相同，则将两个数字写在纸2上。否则将第一个数字加1，如果第一个数字超过999，则重置为1000，并将第二个数字加1。然后用时间转换器将纸2送回过去的房间，成为纸1。可以看出，唯一能够自洽的时间线就是纸1和纸2上都是当初选择的两个质数。这实际上就成了分解因数的一个多项式时间的算法。而分解因数一直被认为是NP问题。如果你对计算机比较了解的话，这一段剧情完全可以让你达到颅内高潮。顺便一提，这个实验的结局是，哈利在纸1上看到了自己的字迹写的“不要和时间开玩笑”，然后用颤抖的手抄到了纸2上。\n扎根原著又超越原著的人物设定 虽然本书的时间线仅仅是原著第一部，但是作者将七本书的元素和人物都已经提前融了进去（毕竟在第一年末尾主角就打败伏地魔了）。这些出场的人物大部分都符合原著，比如海格、麦格、斯内普。而有的人物相比原著却做了大幅度修改。这其中最明显的一个是主角哈利，相比起原著中和他爸爸一样有点鲁莽的哈利·波特，这个哈利·万瑞斯要更加聪明。但是如果你又不得不承认，原著中哈利的天性如果在一个知识分子家庭中成长，最终得到的就是这样一个角色。\n而彻底推翻原著设定的则是大反派伏地魔。原著中的伏地魔是一个只能存在于童话中的反派。他没有自己的理想，除了搞恐怖活动之外他的食死徒群体没有目标。而且以原著中魔法部的禁戒疏散，伏地魔这样不择手段的反派早就应该控制了整个魔法部（只需要对几个高级官员施展摄魂咒）。没有目标则是更大的问题。本书中的汤姆·里德尔则是现实中邪恶人物的极端化。他的邪恶在于他没有任何的同理心，因此他的行为并不是为了享受作恶的快乐，而仅仅是为了达成他的目标。和本书中的哈利一样，他对于死亡有极大的恐惧和厌恶。但是因为缺乏同理心，所以他只想避免自己的死亡，而对他人的死亡无动于衷。为了达成永生的目标，他制作了难以计数的魂器，甚至把其中之一跟着先驱者11号发射到了太阳系外。而且他有充分的智商去制定一个低成本的征服魔法界的计划。他原本打算建立一个大反派人设来吸引魔法界的仇恨，然后再建立一个正义人设来打败自己，从而像邓布利多打败格林德沃一样获得最高的权力和威望。但是没想到整个魔法界在对抗反派人设“伏地魔”的过程中，不断地拖正派人设“大卫·门罗”的后腿，而食死徒的事业则不断高歌猛进。汤姆·里德尔最终忍无可忍，决定用反派人设车翻全世界。\n另外值得一提的是邓布利多。邓布利多是本书中除了哈利和伏地魔之外唯一的聪明人（麦格教授、阿米莉亚·博恩斯、斯内普等人能算半个）。这可能是本书的一个缺陷，但是如果我们坚持原著中的某些设定（魔法部门口没有任何反复方汤剂的措施，霍格沃滋在神秘人归来随时可能发起袭击的时候还在鼓励学院之间竞争学院杯），你就不得不承认魔法世界中的大部分人都蠢得无可救药。但是和哈利和伏地魔的聪明不同，邓布利多表现出来的更像是睿智。他能够比他们两个都看得更加深远。当哈利和奇洛教授还在欣喜于通过麻瓜技术改进的飞天扫帚从傲罗包围中逃走时，邓布利多已经看到了麻瓜的科技被引入巫师战争中的毁灭性后果。正是这种远见使得邓布利多超越了伏地魔。伏地魔被邓布利多有意泄漏的预言牵着鼻子走，而邓布利多却能操纵预言来达成自己想要的未来。\n科幻小说般的精神内核 在第一部分我只是将这本书形容为“带有科幻色彩”，因为本书很难符合一般的科幻小说的定义，因为它既不指向未来，也不描述科技。但是在世界观上，这本书与科幻小说还是表现出了许多相似。作者对魔法的思考一开始就放在了宇宙的尺度下，哈利与奇洛教授真正成为朋友是在奇洛教授用魔法带他体会身处太空的景象。在哈利对魔法本质的探索中，作者也暗示了一种非常科幻的魔法起源：曾经有一个非常强大的文明（亚特兰蒂斯）用难以想象的科技建造了一个自动化的辅助系统。这个系统有一系列实现功能的指令（咒语）。同时这个系统只会听从具有亚特兰蒂斯基因的人（巫师）。\n比这更深一层的是，这本书具有典型的科幻小说的精神内核。奇幻小说的精神内核往往是今不如古，格兰芬多的宝剑、死亡三圣器都是古人制造的威力巨大的魔法物品，今人却再也没有能力制造了。瓦雷利亚人曾经拥有成批的巨龙和大量生产瓦雷利亚钢的能力，今人仅仅是靠着瓦雷利亚文明的少许遗留。威胁中土的恐怖阴影索伦也不过是古代的魔王魔苟斯的爪牙。本书建立在《哈利波特》的世界观下，也不得不承认魔法的力量正在消退这一事实。但是哈利却丝毫没有因此而沮丧。他相信用科学方法来研究魔法能够找到魔法的底层原理。即使魔法衰退了，建立在新的原理上的科技也能把人类带向更美好的未来。\n“好吧，”德拉科说道。他深吸了一口气，声音变得略微低沉，开始抑扬顿挫地演讲。“麻瓜出身的巫师污染了魔法世界，令我们的魔力越来越弱，一代不如一代。曾经萨拉查·斯莱特林和戈德里克·格兰芬多和罗依纳·拉文克劳和赫尔加·赫奇帕奇以他们的力量创建了霍格沃茨，制造了挂坠盒，宝剑，王冠，圣杯这样的魔法宝物，近代的巫师没有一个人能和他们比肩。我们正在变弱，在往麻瓜的方向退化，因为我们和麻瓜出身的人通婚，让没有魔力的哑炮（squib)孩子活下来。如果任这样的污染继续的话，很快我们的魔杖就会折断，我们的艺术就会失传，梅林的传人就会灭绝，亚特兰帝斯的血脉就会从此沉沦。我们的孩子们会像麻瓜们一样在尘埃里刨食，黑暗会永远笼罩这个世界。”德拉科喝了一口茶，看上去很满意；看起来，这就是他的整个的论点了。 “很有说服力，”哈利说，不过他指的是修辞手段，不是内容。这是一个标准的模式：光荣的时代的堕落，为了保护仅存的珍贵部分必须保持纯洁，抗拒污染，过去是上坡路而未来是下坡路。而这个模式也有它的天敌…“不过我必须纠正一个事实。你关于麻瓜的信息有点过时了。我们已经不再在灰尘里刨食了。” 德拉科猛地回过头。“什么？你什么意思，我们？” “我们。我们科学家。弗兰西斯·培根的传人和启蒙运动的血脉。麻瓜们没有呆坐着哭诉没有魔杖的悲哀，我们有我们自己的力量，有没有魔法都没关系。如果你们的力量消失了，对我们大家都是巨大的损失，因为对于我们来说，你们的魔法是指向这个世界的真正规则的唯一线索——但是你们不会在灰尘里刨食的。你们的房子还是会冬暖夏凉，你们还是会有医生和药物。如果魔法失传了，科学会让你活下去。那会是个悲剧，但决不会是世界的光芒熄灭的时刻。我是这么认为的。” 当然还有我认为本书的高光时刻，哈利面对死亡的化身摄魂怪（本书设定），说出了那段代表未来人类的宣言\n我知道你是什么了，哈利想道，魔杖转动了一次，两次，三次，四次，手指滑过了精确的距离，我理解了你的本质，你是死亡的象征，是死亡通过某种魔法规则在这个世界上投下的阴影。 而死亡是我永远不会接受的事情。 它只是年少时的无奈，人类只是还没有完全长大。 有一天…… 我们会摆脱它…… 大家就再也不用互相说再见…… 魔杖举了起来，直指摄魂怪。 “呼神护卫！” 这个想法像决了堤的洪水一样喷薄而出，沿着手臂涌向他的魔杖，杖头迸发出耀眼的白光…… 光芒成形了，产生了形状和物质。 一个拥有两条胳膊，两条腿，一个头颅的形象，直立地站着。这是名为智人的动物，是人类的形象。 总有一天，人类的子孙会遍布各个星球，到那时，他们要等到孩子长大，有足够的承受能力的时候，才会告诉他们远古的地球的历史；而他们在听说的时候，会因为死亡这样的事情竟然曾经存在过而悲伤流泪！ 人类的形象变得比正午的太阳还要夺目，放射出的光芒令哈利的皮肤感到一阵温暖。哈利向死亡的阴影倾泄出了他全部的蔑视，打开了心中所有的闸门，让这个明亮的形象变得越来越耀眼，越来越耀眼。 你不是不可战胜的，总有一天人类会终结你。 如果我能，我会终结你，凭着智慧和魔法和科学的力量。 我不会在死亡的恐惧下颤抖，只要我还有胜利的机会。 我不会任凭死亡碰到我，不会任凭死亡碰到我爱的人们。 即使你真的在我终结你之前终结了我， 也会有人代替我的位置，随后又会有人代替他们的位置， 直到世界的伤口最终被治愈…… 面对死亡的不屈是哈利和伏地魔共有的特点。然而同理心的缺乏使得伏地魔只想逃避他自己的死亡，而哈利却想要为全人类终结死亡。对我来说，这段话就如同《永恒的终结》结尾，哈兰和诺羽决定终结永恒时空，为人类开启走向银河帝国的可能时\n诺羽说道，“这就是地球。这里不仅仅属于永恒组员所有，而是全体人类唯一的故乡，但也是未来无穷冒险的唯一起点。你的任务，就是作出决定。这是你要作出的决定。你和我以及洞穴里头的物品，都会受到物理时间力场的保护，不会受到这次变革的影响。库柏将和那则广告一同消失；永恒时空和我的世纪，也都将一起终结。但我们会留下来，我们两人拥有孩子和孙子，而且人类总有一天能够在群星之间进行太空旅行。” 他转头看着她，而她则回以微笑。这是他一向熟悉的诺羽。 他并不清楚自己是否已作出决定，直到那片亮白突然布满了整个天空。原本倚着天空的时空壶朦胧外形，现在已经全部消失。 发现时空壶的消失，他便晓得了一切。诺羽缓缓地进入了他的怀抱，而且，永恒时空最后终于结束了。 ——但这同时也是无限时空的开启。 --- 《永恒的终结》 第十八章 无限时空的开启 最后值得一提的是，尽管作者当时是在互联网上连载这一小说，但是这本书却有着非常好的前后呼应和伏笔，这使得本书非常适合多次阅读，每次总能有新的发现。\n","date":1582377969,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582377969,"objectID":"657836524c8594804c63fc331607e549","permalink":"https://zxdu.xyz/post/report-hpmor/","publishdate":"2020-02-22T21:26:09+08:00","relpermalink":"/post/report-hpmor/","section":"post","summary":"读书感想","tags":[],"title":"读书感想：《哈利波特与理性之道》","type":"post"},{"authors":["Zhengxiao Du","Jie Tang","Yuhui Ding"],"categories":[],"content":"","date":1569813592,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569813592,"objectID":"0bf068cad51390a710162609fe82a1fb","permalink":"https://zxdu.xyz/publication/tkde-19/","publishdate":"2019-09-30T11:19:52+08:00","relpermalink":"/publication/tkde-19/","section":"publication","summary":"We study the problem of personalized article recommendation, in particular when the user’s preference data is missing or limited, which is knowns as the user cold-start issue in recommender systems. We propose POLAR++, an active recommendation framework that utilizes Bayesian neural networks to capture the uncertainty of user preference, actively selects articles to query the user for feedback, and adaptively learns user preference with one-shot learning. For the article recommendation, we design an attention-based CNN to quantify the similarity between user preference and recommended articles, which signiﬁcantly improves the performance with only a few articles rated by the users. We evaluate the proposed POLAR++ on datasets of different scale and sources. Experimental results demonstrate the effectiveness of the proposed model. We have successfully deployed POLAR++ into AMiner as the recommendation engine for article recommendation, which further conﬁrms the effectiveness of the proposed model.","tags":[],"title":"POLAR++: Active One-shot Personalized Article Recommendation","type":"publication"},{"authors":["Zhengxiao Du","Xiaowei Wang","Hongxia Yang","Jingren Zhou","Jie Tang"],"categories":null,"content":"","date":1563580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563580800,"objectID":"03a2041179dddcfb4791a622e94f5eac","permalink":"https://zxdu.xyz/publication/kdd-19/","publishdate":"2019-07-20T13:59:10.997616Z","relpermalink":"/publication/kdd-19/","section":"publication","summary":"Cold-start problems are long-standing challenges for practical recommendations. Most existing recommendation algorithms rely on extensive observed data and are brittle to recommendation scenarios with few interactions. This paper addresses such problems using few-shot learning and meta learning. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. To accomplish this, we combine the scenario-specific learning with a model-agnostic sequential meta-learning and unify them into an integrated end-toend framework, namely Scenario-specific Sequential Meta learner (or $s^2$ Meta ). By doing so, our meta-learner produces a generic initial model through aggregating contextual information from a variety of prediction tasks while effectively adapting to specific tasks by leveraging learning-to-learn knowledge. Extensive experiments on various real-world datasets demonstrate that our proposed model can achieve significant gains over the state-of-the-arts for cold-start problems in online recommendation. Deployment is at the Guess You Like session, the front page of the Mobile Taobao.","tags":null,"title":"Sequential Scenario-Specific Meta Learner for Online Recommendation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"1f4cb24553577f6808a73065326a8b9d","permalink":"https://zxdu.xyz/blog/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/blog/","section":"","summary":"Zhengxiao's Blog","tags":null,"title":"Blog","type":"widget_page"},{"authors":["Yifeng Zhao","Jie Tang","Zhengxiao Du"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"0289d880c699e53b73f74f6d184dc899","permalink":"https://zxdu.xyz/publication/pakdd-19/","publishdate":"2019-07-20T13:59:10.996845Z","relpermalink":"/publication/pakdd-19/","section":"publication","summary":"","tags":null,"title":"EFCNN: A Restricted Convolutional Neural Network for Expert Finding","type":"publication"},{"authors":["Zhengxiao Du","Jie Tang","Yuhui Ding"],"categories":null,"content":"","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"b4e5b8386cfa4503b44040458c1c82fc","permalink":"https://zxdu.xyz/publication/pkdd-18/","publishdate":"2019-07-20T13:59:10.995108Z","relpermalink":"/publication/pkdd-18/","section":"publication","summary":"In this paper, we propose POLAR, an attention-based CNN combined with one-shot learning for personalized article recommendation. Given a query, POLAR uses an attention-based CNN to estimate the relevance score between the query and related articles. The attention mechanism can help significantly improve the relevance estimation. For example, on AMiner, this can help achieve a +5.0% improvement in terms of NDCG@3. One more challenge in personalized article recommendation is how to collect statistically sufficient training data for a recommendation model. POLAR combines a one-shot learning function into the recommendation model, which further gains significant improvements. For example, on AMiner, with only 1.6 feedbacks on average, POLAR achieves 2.7% improvement by NDCG@3. We evaluate the proposed POLAR on three different datasets: AMiner, Patent, and RARD. Experimental results demonstrate the effectiveness of the proposed model. Recently, we have successfully deployed POLAR into AMiner as the recommendation engine for article recommendation, which further confirms the effectiveness of the proposed model.","tags":null,"title":"POLAR: Attention-Based CNN for One-Shot Personalized Article Recommendation","type":"publication"}]