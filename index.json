
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"I am a PhD student in the Department of Computer Science and Technology at Tsinghua University. I am also a member of the Knowledge Engineering Group, advised by Prof. Jie Tang.\nMy research interest lies in the application of machine learning algorithms to real-world systems, including information retrieval [ECML/PKDD\u0026#39;18,KDD\u0026#39;19,TKDE,SIGIR\u0026#39;19], knowledge graphs [TKDE], and pretrained language models[ACL\u0026#39;21,ACL\u0026#39;21]. Generally, I am also interested in the algorithms for machine learning and reinforcement learning.\nI am currently co-leading the pre-training team of ChatGLM group at ZhipuAI.\nIn my spare time, I like to read science fiction novels and history books.\n","date":1711152000,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1711152000,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://zxdu.xyz/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a PhD student in the Department of Computer Science and Technology at Tsinghua University. I am also a member of the Knowledge Engineering Group, advised by Prof. Jie Tang.","tags":null,"title":"Zhengxiao Du","type":"authors"},{"authors":["Zhengxiao Du"],"categories":["技术"],"content":"最近因为研究需要学习了 vector quantization（VQ）并且实际训练了一些带有 VQ bottleneck 的模型。在这里记录一下学习和训练的心得。\n什么是 VQ 我们知道神经网络强大的表示能力很大程度上来自于其使用的连续向量表示，以及依赖于连续向量表示的梯度下降优化算法。而 vector quantization 则是将神经网络中某一层的连续向量表示替换为离散变量。具体方式为有一个词表（codebook） $e\\in R^{K\\times D}$，其中$K$ 是词表大小，$D$ 是向量维度。对于神经网络中某一层的输出 $z\\in R^{D}$，我们希望找到一个词表中的向量 $e_i$ 使得 $e_i$ 与 $z$ 的距离最小，即 $i = \\arg\\min_j |x - e_j|_2$。这样，我们就可以将 $e_i$ 作为 $z$ 的近似，称为 $z_q$，继续参与后面的运算。这种替换方式可以看作是一种量化（quantization）操作，因此得名 vector quantization。\n为什么需要 VQ VQ 最为知名的应用就是 VQVAE，常常被用于将原始图片编码为离散的 token 序列。但是实际上 VQ 可以用于任何神经网络中，并不局限于 AutoEncoder。\nVQ 可以看作是 Transformer 中 embedding 层的逆运算。Embedding 层的作用就是将神经网络不好处理的离散变量转化为连续向量表示，为什么我们还需要反过来呢？离散变量的优点在于：\n更适合作为生成任务的目标。我们已经知道自回归 Transformer 能够很好地建模自然语言这样的离散序列。而连续向量序列，比如原始的图片、音频等，往往需要 diffusion model 这样更复杂的生成模型来建模。 节省存储空间。连续变量 $z$ 需要存储 $D$ 个浮点数，而离散变量 $e_i$ 只需要存储一个整数。 VQ 的训练 可以看出带有 VQ bottleneck 的优化并不容易。原因在于将 $z$ 转化为 $z_q$ 时存在一个 $\\arg\\min$ 操作，它是不可导的。\n为了方便，我们把神经网络在 VQ bottleneck 前的部分看作 encoder，其作用是将输入 $x$ 转化成连续向量 $z$，把 VQ bottleneck 后的部分称为 decoder，其作用是从量化后的向量 $z_q$ 中解码出需要的输出。带有 VQ bottleneck的神经网络的参数分为三部分，即 encoder 的参数 $\\theta_{\\text{enc}}$，decoder 的参数 $\\theta_{\\text{dec}}$，以及词表 $e$。decoder 的参数的优化不存在任何问题，因为它是以 $z_q$ 作为输入的，$z_q$ 是怎么来的与它无关，只需要优化神经网络原来的 loss即可。而 encoder 的参数则问题很大，因为它以 $z$ 作为输出， decoder 的梯度回传只能到 $z_q$，而 $z$ 到 $z_q$ 是不存在梯度的。在VQVAE中提出了一个近似方法，即用 $z_q$ 的梯度作为 $z$ 的梯度。原因在于两者的维度都是 $D$，并且 $z_q$ 是 $z$ 的近似，因此它们的梯度应该是相似的。给定 decoder 的输出目标 $y$，假设原来的 loss 为 $l(y, \\text{Dec}(z))$，则加上 VQ bottleneck 之后的 loss 为\n$$ l(y,\\text{Dec}(z+sg(z_q-z))) $$\n其中 $sg$ 表示 stop gradient，即在 backward 的时候不计算着一部分的梯度。在 PyTorch 中实现这个近似也比较容易，只要用 z+(z_q-z).detach() 替代 z_q 进行后续的 forward 计算即可。\n通过以上方法我们优化了 encoder 和 decoder 部分，但是还没有优化 VQ 过程。VQ 过程的优化目标是使量化后的表示 $z_q$ 尽可能地接近 $z$。因此我们直接优化 $z_q$ 和 $z$ 的 mean-squared-error loss，即 $|z_q-z|_2^2$。这个 loss 可以进一步拆分为两部分，即 $|z_q-sg(z)|_2^2$ 和 $|z-sg(z_q)|_2^2$。前者使得 codebook 中离 $z$ 最近的向量进一步接近 $z$，后者使得 $z$ 进一步接近 codebook 中离它最近的向量。相对来说，我们更希望 codebook 中的向量去接近 $z$，因此前者的权重更大一些。\n最终我们优化的 loss 为 $$ l(y, \\text{Dec}(z+sg(z_q-z)))+\\alpha |z_q-sg(z)|_2^2+\\beta |z-sg(z_q)|_2^2 $$\n在整个 loss 中，只有第二项的 loss 与 codebook 的更新有关，其实也可以直接得到一个 closed-form optimal。假设 ${z_{i,1},z_{i,2},\\cdots,z_{i,n_i}}$ 为 encoder 输出中离散近似为 $e_i$ 的 $n_i$个向量，则根据 MSE loss 的性质 $e_i$ 最优解为 $$ e_i = \\frac{1}{n_i}\\sum_{j=1}^{n_i}z_{i,j} $$\n不过实际中我们都是使用 minibatch 进行训练，在每一步优化时只能看到 batch 中的样本。因此我们使用 exponential moving average（EMA）来进行更新 $$ N_i^{(t)}=N_i^{(t-1)}\\gamma+n_i^{(t)}(1-\\gamma)\\ m_i^{(t)}=m_i^{(t-1)}\\gamma+\\sum_jz_{i,j}^{(t)}(1-\\gamma) $$\n使用EMA更新 codebook 时，loss可以改为 $$ l(y, \\text{Dec}(z+sg(z_q-z))+\\beta |z-sg(z_q)|_2^2 $$\nVQ 的训练心得 codebook 的初始化非常重要 无论是否使用 EMA，codebook 中都只有被激活的向量才会被更新。因此如果 codebook 中某些向量从一开始就没有激活，那么这些向量就一直不会被更新，导致使用的 codebook 大小过小。之前很多研究表明预训练模型中连续向量的分布并不是在整个欧几里得空间上均匀的，而是集中分布在一个较小的区域。因此完全随机初始化的 codebook 往往只有一小部分向量会被激活。\n超参数选择的意义 使用 EMA 更新的方法相当于直接计算了 codebook 最优解，收敛速度会更快。$\\gamma$ 决定了 codebook 更新的速率，$\\gamma$ 越小 codebook 更新越快。\n系数为 $\\beta$ 的 loss 项其实是对 $z$ 的一个正则项。$z$ 的复杂度是由 encoder 的参数量决定的，而 $z_q$ 的复杂度是由 codebook 的大小决定的，实际中 $z$ 的复杂度一般远大于 $z_q$。如果不加约束的话 $z_q$ 永远也追不上 $z$。而用 $z_q$ 的梯度近似 $z$ 的梯度要求 $z$ 和 $z_q$ 必须非常接近。因此需要用 $\\beta$ 来约束 $z$ 的复杂度。\n如何防止 codebook collapse 在学习过程中 $z$ 的分布是始终在变化中的，即使初始化的时候 codebook 中的大部分向量都能被激活，随着训练的进行，codebook 中被激活的向量也可能会越来越少。这个现象被称为 codebook collapse。通过仔细设置 $\\gamma$, $\\beta$ 和 学习率可以缓解这一问题，但是会非常麻烦。\n另一个想法是，我们可以重新初始化 codebook 中长时间没有被激活的向量，这个想法我是在 Jukebox: A Generative Model for Music 中看到的。使用 EMA 更新的时候 $N_i^{(t)}$ 实际上已经记录了向量 $e_i$ 在过去被激活次数的 exponential moving average，可以用来判断有哪些向量长期没有被激活。至于如何重新初始化这些向量，我们希望重新初始化之后这些向量能够被激活，因此我们可以用当前 batch 中的 $z$ 来重新初始化这些向量。\n","date":1725120000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1725120000,"objectID":"dde1b0e0104b27e56e9079a5ff73b211","permalink":"https://zxdu.xyz/post/vector-quantize/","publishdate":"2024-09-01T00:00:00+08:00","relpermalink":"/post/vector-quantize/","section":"post","summary":"从零学习 Vector Quantization","tags":[],"title":"从零学习 Vector Quantization","type":"post"},{"authors":["Zhengxiao Du"],"categories":["技术"],"content":"原始的 Multi-Head Attention 的计算公式为（单个head，暂时先不考虑 RoPE）\n$$ Q_i=W^Q_iH $$\n$$ K_i=W_i^KH $$ $$ V_i=W^V_iH $$ $$ O_i=\\text{Softmax}(\\frac{Q_i^TK_i}{\\sqrt{d_h}})V_i $$ 其中$H$ 是 $d\\times l$ 的矩阵（$l$ 为序列长度），表示 hidden states， $W^Q_i, W^K_i, W^V_i$ 是 $d_h\\times d$的矩阵。\nMulti-Query Attention 的计算公式为（单个head，暂时先不考虑 RoPE） $$ Q_i=W^Q_iH $$\n$$ K=W^KH $$ $$ V=W^VH $$ $$ O_i=Softmax(\\frac{Q_i^TK}{\\sqrt{d_h}})V_i $$ 最大的区别在于 $K,V$ 是所有 head共享的，因此能够减少KV Cache的显存占用。其中 $$ Q_i^TK=H^T(W_i^Q)^TW^KH $$ Multi-Head Latent Attention 的计算公式为（单个head，暂时先不考虑 RoPE） $$ C^Q=W^{DQ}H $$ $$ Q_i=W^{UQ}_iC^Q $$ $$ C^{KV}=W^{DKV}H $$ $$ K_i=W_i^{UK}C^{KV} $$ $$ V_i=W_i^{UV}C^{KV} $$ $$ O_i=Softmax(\\frac{Q_i^TK_i}{\\sqrt{d_h}})V_i $$ 其中 $W^{DQ}, W^{DKV}\\in \\mathbb{R}^{d_c\\times d}$，$W_i^{UQ},W_i^{UK},W_i^{UV}\\in\\mathbb{R}^{d_h\\times d_c}$\n单独看 Attention 计算的前一部分，其中 $$ Q_i^TK_i=H^T(W^{DQ})^T(W_i^{UQ})^TW^{UK}_iW^{DKV}H $$ 令 $W_i^Q=(W_i^{UK})^TW_i^{UQ}W^{DQ}$，我们有 $$ Q_i^TK_i=H^T(W_i^Q)^TW^{DKV}H $$ 可以看到这一计算公式和 Multi-Query Attention 其实是一样的，都是使用的单独的 $Q$ 和共享的 $K$。区别在于，这里 $W_i^QH,W^{DKV}H\\in\\mathbb{R}^{d_c\\times l}$。也就是说在进行 attention 计算的时候，向量点积的维度是 $d_c$ 而不是 $d$。在论文中实际设置的是 $d_c=4d$。也就是说 Multi-Head Latent Attention 其实是 head dimension 提高到4倍的 Multi-Query Attention。在论文中也提到了在 inference 的时候 absorb $W^{UK}$ into $W^{UQ}$，其实就代表了这里的结合方式。因为每个head的维度提高了，所以能够计算出更加复杂的 attention分布，从而相比起 Multi-Query Attention 取得性能提升。相比起直接提高 head dimension，其优点在于所有head的 $W^{DQ},W^{UQ},W^{UK}$的总参数量是 $d\\cdot d_c+d \\cdot d_c+ d \\cdot d_c=3d\\cdot d_c=12d\\cdot d_h$，而所有 head 的 $W^Q$ 的参数量是 $d \\cdot d_c\\cdot n_h=4d^2$，节省了参数量。也就是说对 $W^Q$ 做了一个低秩分解。\n但是这个提升并不是 free lunch，因为 head dimension 提高意味着 attention 的计算量也提高，而 attention 的计算量是 $O(l^2)$ 的。为了处理长文本，现在大家一般都倾向于尽可能降低 attention 计算量的常数，而这个方法是会增加常数的。\n以上分析没有考虑 RoPE，如果考虑 RoPE 的话，每个 head 的维度会从 $4d$ 变成 $4.5d$，其中$4d$是没有 positional encoding的，$0.5d$ 是使用 RoPE encoding的。其实 ChatGLM2-6B 中已经使用过类似的做法，即只在一半的 head dimension 上使用 RoPE ，目的是为了把 attention 计算分成位置相关和位置无关的两部分，与性能提升的关系并不大。\n","date":1715261169,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1715334909,"objectID":"52df302f05bd06530a527cd80f3aa7b1","permalink":"https://zxdu.xyz/post/mhla/","publishdate":"2024-05-09T21:26:09+08:00","relpermalink":"/post/mhla/","section":"post","summary":"关于 MHLA的一些分析","tags":[],"title":"关于 MHLA（Multi-Head Latent Attention）的一些分析","type":"post"},{"authors":["Zhengxiao Du","Aohan Zeng","Yuxiao Dong","Jie Tang"],"categories":null,"content":"","date":1711152000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1711152000,"objectID":"0ecabccefebb59c54faa08fa4df2292e","permalink":"https://zxdu.xyz/publication/neurips-24/","publishdate":"2024-03-23T00:00:00Z","relpermalink":"/publication/neurips-24/","section":"publication","summary":"Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks -- regardless of the continuity of metrics -- when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.","tags":null,"title":"Understanding Emergent Abilities of Language Models from the Loss Perspective","type":"publication"},{"authors":["Aohan Zeng","Xiao Liu","Zhengxiao Du","Zihan Wang","Hanyu Lai","Ming Ding","Zhuoyi Yang","Yifan Xu","Wendi Zheng","Xiao Xia","Weng Lam Tam","Zixuan Ma","Yufei Xue","Jidong Zhai","Wenguang Chen","Peng Zhang","Yuxiao Dong","Jie Tang"],"categories":null,"content":"","date":1664928000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664928000,"objectID":"e40d759a9572efd2a784d080f7afe364","permalink":"https://zxdu.xyz/publication/glm-130b/","publishdate":"2022-10-05T00:00:00Z","relpermalink":"/publication/glm-130b/","section":"publication","summary":"We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B","tags":null,"title":"GLM-130B: An Open Bilingual Pre-Trained Model","type":"publication"},{"authors":["Zhengxiao Du"],"categories":["技术"],"content":"前言 密码是我们在互联网上使用各种服务的时候很难绕过的一环。我们经常能听到各种各样的科普文章中告诫我们，为了保护好自己的互联网账号，密码一定要足够复杂，最好能把大小写字母、数字、特殊符号都用上。但是，复杂的密码意味着难以记忆，而当下一个普通的互联网用户很可能拥有几十个账户。为了不折磨自己的脑细胞，很多人选择给多个账户使用同一个密码。然而，由于有的网站在数据安全方面疏于防护，用户的密码遭到泄漏的事故时有发生。虽然这些密码遭到泄漏的账户可能没有很高的价值，但是却可能导致使用了同样密码的高价值账户被盗。此外，有的人还在创建复杂密码的时候还会使用自己的姓名缩写、生日、手机号等信息，这也为通过社会工程学手段破解密码提供了可能。\n为了解决这一问题，密码管理服务应运而生。密码管理服务为用户提供了存储互联网账户的用户名和密码的服务，往往还会和浏览器、手机操作系统等结合，提供自动填充、自动记录等功能。在安全性方面，无论是存储在本地还是云端，都会用用户设置的密码进行加密。有了密码管理服务，我们就可以给每个互联网账户使用随机生成的强密码。最常见的密码管理服务就是Chrome、Edge等浏览器自带的密码自动填充功能和iOS、MacOS提供的系统级密码管理。我之前使用的也是Chrome的自动填充功能，但是这个功能毕竟是绑定了Chrome浏览器和谷歌账号。这就限制了浏览器的选择，比如前段时间使用Chromium内核的Edge很火，但是因为自己的互联网账号密码都保存在Chrome上，所以无法尝试。类似的，苹果的密码管理服务也不方便在Windows和Android下使用。因此我最近决定迁移到相对独立的密码管理软件上。\n为什么选择BitWarden 密码管理服务的安全性来自于现代加密算法，在无法攻破加密密码的情况下是无法获得加密内容的。但是为了能够在不同的设备上使用保管的密码，我们必须依赖于云端服务。而一旦涉及到云服务，就存在着很大的不确定性，因为我们很难检查服务器上实际运行的代码。这也是我没有选择很热门的1Password的原因。\nBitWarden是前后端均开源的密码管理服务器，并且还支持由用户自己搭建后端服务。当然实际中大家使用更多的是Vaultwarden（原名为BitWarden_RS）这个非官方简化版后端实现，因为它占用的资源更少。同时BitWarden的前端也有很好的跨平台支持，对主流的浏览器和操作系统都提供了支持。\n如何搭建自己的Vaultwarden服务 首先用docker容器运行Vaultwarden服务\ndocker pull vaultwarden/server:latest mkdir /vw-data # this is used for permanent data storage docker run -d --name vaultwarden \\ -e WEBSOCKET_ENABLED=true \\ -v /vw-data/:/data/ \\ -p 10020:80 -p 3012:3012 \\ vaultwarden/server:latest 其中10020和3012分别是主服务和WebSocket服务的端口，/vw-data是用来保存数据的文件夹。\n然后在域名服务商的管理页面添加DNS记录，将准备的域名指向服务器的IP地址。作为例子，这里假设我们准备的域名是www.example.com，服务器的IP是1.1.1.1。\n然后安装certbot\napt-get install -y snapd snap install core; sudo snap refresh core snap install --classic certbot ln -s /snap/bin/certbot /usr/bin/certbo 然后使用certbot进行证书生成\nwget https://github.com/joohoi/acme-dns-certbot-joohoi/raw/master/acme-dns-auth.py chmod +x acme-dns-auth.py # 注意要更改acme-dns-auth.py的首行的python为python3 mv acme-dns-auth.py /etc/letsencrypt/ certbot certonly --manual --manual-auth-hook /etc/letsencrypt/acme-dns-auth.py --preferred-challenges dns --debug-challenges -d \\*.www.example.com -d www.example.com 在最后一步会要求你在域名的DNS配置中添加一个CNAME项来认证你的所有权，完成之后等待DNS更新之后按下回车。如果还没有更新完成就回车的话认证会失败，可以重新运行这一命令。证书相关的文件保存在/etc/letsencrypt/live/www.example.com下面。\n最后在/etc/nginx/conf.d/vaultwarden.conf中进行nginx配置（需要先安装nginx）\n# Redirect HTTP to HTTPS server { listen 80; listen [::]:80; server_name www.example.com; return 301 https://$host$request_uri; } server { listen 443 ssl http2; listen [::]:443 ssl http2; server_name www.example.com; ssl_certificate /etc/letsencrypt/live/www.example.com/fullchain.pem; ssl_certificate_key /etc/letsencrypt/live/www.example.com/privkey.pem; # Specify SSL config if using a shared one. #include conf.d/ssl/ssl.conf; # Allow large attachments client_max_body_size 128M; location / { proxy_pass http://127.0.0.1:10020; proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; } location /notifications/hub { proxy_pass http://127.0.0.1:3012; proxy_set_header Upgrade $http_upgrade; proxy_set_header Connection \u0026#34;upgrade\u0026#34;; } location /notifications/hub/negotiate { proxy_pass http://127.0.0.1:10020; } } 然后就可以在www.example.com中访问自己的BitWarden服务了。在使用各个平台的BitWarden插件和客户端的时候，要在登陆之前先进行设置，将服务地址修改为www.example.com。\n更新Vaultwarden镜像 # Pull the latest version docker pull vaultwarden/server:latest # Stop and remove the old container docker stop vaultwarden docker rm vaultwarden # Start new container with the data mounted docker run -d --name vaultwarden \\ -e WEBSOCKET_ENABLED=true \\ -v /vw-data/:/data/ \\ -p 10020:80 -p 3012:3012 \\ vaultwarden/server:latest 参考文献 Installing Vaultwarden formally bitwarden_rs on Ubuntu 20.04 with Nginx. https://www.llewellynhughes.co.uk/post/installing-vaultwarden Vaultwarden Wiki. https://github.com/dani-garcia/vaultwarden/wiki ","date":1657467300,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1657467300,"objectID":"f4e8cfebdb579bcfef5369a6d0c5f6d5","permalink":"https://zxdu.xyz/post/bitwarden/","publishdate":"2022-07-10T23:35:00+08:00","relpermalink":"/post/bitwarden/","section":"post","summary":"Vaultwarden tutorial","tags":[],"title":"用Vaultwarden搭建自己的密码管理服务","type":"post"},{"authors":["Zhengxiao Du","Yujie Qian","Xiao Liu","Ming Ding","Jiezhong Qiu","Zhilin Yang","Jie Tang"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"c5af62dffbcd378227f2f3810518155b","permalink":"https://zxdu.xyz/publication/glm-21/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/publication/glm-21/","section":"publication","summary":"There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks differ in nature, with three main categories being natural language understanding (NLU), unconditional generation, and conditional generation, while none of the pretraining frameworks performs the best for all tasks. We propose a General Language Model (GLM)  based on autoregressive blank infilling to address this challenge. The proposed architecture has two major benefits: (1) It improves pretrain-finetune consistency via cloze-style finetuning and naturally handles variable-length blank infilling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pretraining data and steps. (2) It is flexible enough to handle various NLP tasks with a single pretrained model. GLM with 1.25x parameters of BERT-Large achieves the best performance in NLU, conditional and unconditional generation at the same time, demonstrating its generalizability to different downstream tasks.","tags":null,"title":"GLM: General Language Model Pretraining with Autoregressive Blank Infilling","type":"publication"},{"authors":["Xiao Liu","Kaixuan Ji","Yicheng Fu","Weng Lam Tam","Zhengxiao Du","Zhilin Yang","Jie Tang"],"categories":null,"content":"","date":1647302400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1647302400,"objectID":"60431ca2d54a82f8ec6c5a10313bc89f","permalink":"https://zxdu.xyz/publication/p-tuning-v2/","publishdate":"2022-03-15T00:00:00Z","relpermalink":"/publication/p-tuning-v2/","section":"publication","summary":"Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.","tags":null,"title":"P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks","type":"publication"},{"authors":["Zhengxiao Du","Chang Zhou","Jiangchao Yao","Teng Tu","Letian Cheng","Hongxia Yang","Jingren Zhou","Jie Tang"],"categories":null,"content":"","date":1628726400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1628726400,"objectID":"06040017fb42f695dbd287fb1ba81fa8","permalink":"https://zxdu.xyz/publication/cogkr-19/","publishdate":"2021-08-12T00:00:00Z","relpermalink":"/publication/cogkr-19/","section":"publication","summary":"Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-art approaches.","tags":null,"title":"CogKR: Cognitive Graph for Multi-hop Knowledge Reasoning","type":"publication"},{"authors":["Himank Yadav","Zhengxiao Du","Thorsten Joachims"],"categories":null,"content":"","date":1625961600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625961600,"objectID":"91e7107418f0ee03339fca8b6d069ba8","permalink":"https://zxdu.xyz/publication/fair-19/","publishdate":"2021-07-11T00:00:00Z","relpermalink":"/publication/fair-19/","section":"publication","summary":"While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and attractive source of data for learning to rank, it can produce unfair ranking policies for both exogenous and endogenous reasons. Exogenous reasons typically manifest themselves as biases in the training data, which then get reflected in the learned ranking policy and often lead to rich-get-richer dynamics. Moreover, even after the correction of such biases, reasons endogenous to the design of the learning algorithm can still lead to ranking policies that do not allocate exposure among items in a fair way. To address both exogenous and endogenous sources of unfairness, we present the first learning-to-rank approach that addresses both presentation bias and merit-based fairness of exposure simultaneously. Specifically, we define a class of amortized fairness-of-exposure constraints that can be chosen based on the needs of an application, and we show how these fairness criteria can be enforced despite the selection biases in implicit feedback data. The key result is an efficient and flexible policy-gradient algorithm, called FULTR, which is the first to enable the use of counterfactual estimators for both utility estimation and fairness constraints. Beyond the theoretical justification of the framework, we show empirically that the proposed algorithm can learn accurate and fair ranking policies from biased and noisy feedback. ","tags":null,"title":"Policy-Gradient Training of Fair and Unbiased Ranking Functions","type":"publication"},{"authors":["Xiao Liu","Yanan Zheng","Zhengxiao Du","Ming Ding","Jiezhong Qiu","Zhilin Yang","Jie Tang"],"categories":null,"content":"","date":1616025600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1616025600,"objectID":"3f1a92fc0bd524a9c799eb0c0c74a76d","permalink":"https://zxdu.xyz/publication/p-tuning/","publishdate":"2021-03-18T00:00:00Z","relpermalink":"/publication/p-tuning/","section":"publication","summary":"While GPTs with traditional ﬁne-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuningwhich employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we ﬁnd that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.","tags":null,"title":"GPT Understands, Too","type":"publication"},{"authors":["Zhengxiao Du"],"categories":["阅读"],"content":"为了记录自己最近看的书决定开两个专栏，一个是读书笔记，用来记录偏技术性的读书总结，一个是读书感想，用来发表非技术性的读书体会。本篇比较接近一篇安利贴。\n这是一本《哈利波特》的同人小说。《哈利波特》的原著是我阅读次数最多的小说之一，当然都是在初中时期看的。我觉得这也是比较容易欣赏这本同人的要求：对原著小说有一定的了解，但是又没有热爱到不允许出现任何和原著不符的情节和人物。\n当下大部分的《哈利波特》同人无外乎是两种类型，一种是把自己当成原著剧情的补充和扩展，专注于原著主线之外的情节和人物关系；另一种则是只保留原著的世界观，但是讲述完全不同的故事，甚至连主人公都变了（《舌尖上的霍格沃滋》）。而本书则恰好介于两者之间：本书的剧情完全基于原著第一部《魔法石》，没有引入任何新的人物，但是整个情节发展却完全不同。不同在于，如果说原著小说是给儿童的童话故事，而本书则是给成年人的奇幻小说。\n用理性精神探索魔法世界 本书的世界线与原著的最明显不同在于，由于某些原因，哈利的姨妈并没有嫁给德思礼，而是嫁给了牛津大学的物理学教授万瑞斯。哈利从小就收到了万瑞斯教授的呵护和教育，收到了良好的理性精神和科学方法的训练。而在他十一岁的时候，他像原著一样收到了猫头鹰寄来的霍格沃滋的录取通知书。就这样，一个有着科学家头脑的男孩走入了充满不合理的魔法世界。\n因为这种科学世界观被魔法世界冲击的感受，本书带上了浓厚的科幻色彩。当哈利见到麦格教授在他面前变成一只猫时，他因为能量守恒被打破和猫的大脑居然能承载人的思想而跳了起来。值得注意的是，本书并没有强行用现有的科学知识去解释魔法，而是很快承认了魔法的存在打破了现有的科学定理。但是，虽然已有的知识被打破了，但是发现这些知识的方法却没有失效。哈利的人生理想很快变成了用科学的方法论来探索魔法的原理，从而实现自己改造世界的目标。\n在哈利不断接触魔法世界的过程中，作者也为原著中稍显不合理的设定增加了更为自洽和合理的解释。这一类情节的高峰就是时间转换器的相关情节。对原著比较了解的人都知道，时间转换器是原文中非常严重的一个bug。它的能力过于强大，以至于罗琳也不得不在第五部将它们全部毁掉。时间转换器涉及的是科幻小说中一个经典的主题：回到过去。这里作者采用的是“时间线唯一且过去不可改变”的设定，即不存在平行宇宙，并且回到过去无法改变过去。未来的人回到过去的所作所为构成了过去的一部分。如哈利所说，这意味着整个世界是一个超图灵机，因为当下的计算可以利用来自未来的信息。这一设定可谓科幻味十足，并且充满了可利用的空间。对于未来的自己来说，已经知道的事实是无法改变的，但是还不知道的事实却可以回到过去加以影响。因此只要自己不知道某个人是否已经知道了某条信息，就可以利用时间转换器回到过去，将这条信息告诉他/她。\n对于受过科学训练的人来说，另一个剧情上的吸引力在于用科学方法去探索和解释未知世界。这一类情节的高峰同样是关于时间转换器。当得知关于时间转换器的上述设定之后，哈利立刻意识到，这种超图灵机结构完全可以被用来构建解决NP问题的高效算法。于是他进行了如下实验：请一个朋友随机选择100-999之间的两个质数，将两者的乘积告诉自己。然后自己回到房间，捡起未来的自己写给自己的纸条（称为纸1）。然后从笔记本上撕下一张纸（称为纸2）。根据纸1的内容在纸2上书写。如果纸1为空，则在纸一上写下“100 100”。否则将纸1上的两个数字相乘，如果乘积与之前的乘积相同，则将两个数字写在纸2上。否则将第一个数字加1，如果第一个数字超过999，则重置为1000，并将第二个数字加1。然后用时间转换器将纸2送回过去的房间，成为纸1。可以看出，唯一能够自洽的时间线就是纸1和纸2上都是当初选择的两个质数。这实际上就成了分解因数的一个多项式时间的算法。而分解因数一直被认为是NP问题。如果你对计算机比较了解的话，这一段剧情完全可以让你达到颅内高潮。顺便一提，这个实验的结局是，哈利在纸1上看到了自己的字迹写的“不要和时间开玩笑”，然后用颤抖的手抄到了纸2上。\n扎根原著又超越原著的人物设定 虽然本书的时间线仅仅是原著第一部，但是作者将七本书的元素和人物都已经提前融了进去（毕竟在第一年末尾主角就打败伏地魔了）。这些出场的人物大部分都符合原著，比如海格、麦格、斯内普。而有的人物相比原著却做了大幅度修改。这其中最明显的一个是主角哈利，相比起原著中和他爸爸一样有点鲁莽的哈利·波特，这个哈利·万瑞斯要更加聪明。但是如果你又不得不承认，原著中哈利的天性如果在一个知识分子家庭中成长，最终得到的就是这样一个角色。\n而彻底推翻原著设定的则是大反派伏地魔。原著中的伏地魔是一个只能存在于童话中的反派。他没有自己的理想，除了搞恐怖活动之外他的食死徒群体没有目标。而且以原著中魔法部的禁戒疏散，伏地魔这样不择手段的反派早就应该控制了整个魔法部（只需要对几个高级官员施展摄魂咒）。没有目标则是更大的问题。本书中的汤姆·里德尔则是现实中邪恶人物的极端化。他的邪恶在于他没有任何的同理心，因此他的行为并不是为了享受作恶的快乐，而仅仅是为了达成他的目标。和本书中的哈利一样，他对于死亡有极大的恐惧和厌恶。但是因为缺乏同理心，所以他只想避免自己的死亡，而对他人的死亡无动于衷。为了达成永生的目标，他制作了难以计数的魂器，甚至把其中之一跟着先驱者11号发射到了太阳系外。而且他有充分的智商去制定一个低成本的征服魔法界的计划。他原本打算建立一个大反派人设来吸引魔法界的仇恨，然后再建立一个正义人设来打败自己，从而像邓布利多打败格林德沃一样获得最高的权力和威望。但是没想到整个魔法界在对抗反派人设“伏地魔”的过程中，不断地拖正派人设“大卫·门罗”的后腿，而食死徒的事业则不断高歌猛进。汤姆·里德尔最终忍无可忍，决定用反派人设车翻全世界。\n另外值得一提的是邓布利多。邓布利多是本书中除了哈利和伏地魔之外唯一的聪明人（麦格教授、阿米莉亚·博恩斯、斯内普等人能算半个）。这可能是本书的一个缺陷，但是如果我们坚持原著中的某些设定（魔法部门口没有任何反复方汤剂的措施，霍格沃滋在神秘人归来随时可能发起袭击的时候还在鼓励学院之间竞争学院杯），你就不得不承认魔法世界中的大部分人都蠢得无可救药。但是和哈利和伏地魔的聪明不同，邓布利多表现出来的更像是睿智。他能够比他们两个都看得更加深远。当哈利和奇洛教授还在欣喜于通过麻瓜技术改进的飞天扫帚从傲罗包围中逃走时，邓布利多已经看到了麻瓜的科技被引入巫师战争中的毁灭性后果。正是这种远见使得邓布利多超越了伏地魔。伏地魔被邓布利多有意泄漏的预言牵着鼻子走，而邓布利多却能操纵预言来达成自己想要的未来。\n科幻小说般的精神内核 在第一部分我只是将这本书形容为“带有科幻色彩”，因为本书很难符合一般的科幻小说的定义，因为它既不指向未来，也不描述科技。但是在世界观上，这本书与科幻小说还是表现出了许多相似。作者对魔法的思考一开始就放在了宇宙的尺度下，哈利与奇洛教授真正成为朋友是在奇洛教授用魔法带他体会身处太空的景象。在哈利对魔法本质的探索中，作者也暗示了一种非常科幻的魔法起源：曾经有一个非常强大的文明（亚特兰蒂斯）用难以想象的科技建造了一个自动化的辅助系统。这个系统有一系列实现功能的指令（咒语）。同时这个系统只会听从具有亚特兰蒂斯基因的人（巫师）。\n比这更深一层的是，这本书具有典型的科幻小说的精神内核。奇幻小说的精神内核往往是今不如古，格兰芬多的宝剑、死亡三圣器都是古人制造的威力巨大的魔法物品，今人却再也没有能力制造了。瓦雷利亚人曾经拥有成批的巨龙和大量生产瓦雷利亚钢的能力，今人仅仅是靠着瓦雷利亚文明的少许遗留。威胁中土的恐怖阴影索伦也不过是古代的魔王魔苟斯的爪牙。本书建立在《哈利波特》的世界观下，也不得不承认魔法的力量正在消退这一事实。但是哈利却丝毫没有因此而沮丧。他相信用科学方法来研究魔法能够找到魔法的底层原理。即使魔法衰退了，建立在新的原理上的科技也能把人类带向更美好的未来。\n“好吧，”德拉科说道。他深吸了一口气，声音变得略微低沉，开始抑扬顿挫地演讲。“麻瓜出身的巫师污染了魔法世界，令我们的魔力越来越弱，一代不如一代。曾经萨拉查·斯莱特林和戈德里克·格兰芬多和罗依纳·拉文克劳和赫尔加·赫奇帕奇以他们的力量创建了霍格沃茨，制造了挂坠盒，宝剑，王冠，圣杯这样的魔法宝物，近代的巫师没有一个人能和他们比肩。我们正在变弱，在往麻瓜的方向退化，因为我们和麻瓜出身的人通婚，让没有魔力的哑炮（squib)孩子活下来。如果任这样的污染继续的话，很快我们的魔杖就会折断，我们的艺术就会失传，梅林的传人就会灭绝，亚特兰帝斯的血脉就会从此沉沦。我们的孩子们会像麻瓜们一样在尘埃里刨食，黑暗会永远笼罩这个世界。”德拉科喝了一口茶，看上去很满意；看起来，这就是他的整个的论点了。 “很有说服力，”哈利说，不过他指的是修辞手段，不是内容。这是一个标准的模式：光荣的时代的堕落，为了保护仅存的珍贵部分必须保持纯洁，抗拒污染，过去是上坡路而未来是下坡路。而这个模式也有它的天敌…“不过我必须纠正一个事实。你关于麻瓜的信息有点过时了。我们已经不再在灰尘里刨食了。” 德拉科猛地回过头。“什么？你什么意思，我们？” “我们。我们科学家。弗兰西斯·培根的传人和启蒙运动的血脉。麻瓜们没有呆坐着哭诉没有魔杖的悲哀，我们有我们自己的力量，有没有魔法都没关系。如果你们的力量消失了，对我们大家都是巨大的损失，因为对于我们来说，你们的魔法是指向这个世界的真正规则的唯一线索——但是你们不会在灰尘里刨食的。你们的房子还是会冬暖夏凉，你们还是会有医生和药物。如果魔法失传了，科学会让你活下去。那会是个悲剧，但决不会是世界的光芒熄灭的时刻。我是这么认为的。” 当然还有我认为本书的高光时刻，哈利面对死亡的化身摄魂怪（本书设定），说出了那段代表未来人类的宣言\n我知道你是什么了，哈利想道，魔杖转动了一次，两次，三次，四次，手指滑过了精确的距离，我理解了你的本质，你是死亡的象征，是死亡通过某种魔法规则在这个世界上投下的阴影。 而死亡是我永远不会接受的事情。 它只是年少时的无奈，人类只是还没有完全长大。 有一天…… 我们会摆脱它…… 大家就再也不用互相说再见…… 魔杖举了起来，直指摄魂怪。 “呼神护卫！” 这个想法像决了堤的洪水一样喷薄而出，沿着手臂涌向他的魔杖，杖头迸发出耀眼的白光…… 光芒成形了，产生了形状和物质。 一个拥有两条胳膊，两条腿，一个头颅的形象，直立地站着。这是名为智人的动物，是人类的形象。 总有一天，人类的子孙会遍布各个星球，到那时，他们要等到孩子长大，有足够的承受能力的时候，才会告诉他们远古的地球的历史；而他们在听说的时候，会因为死亡这样的事情竟然曾经存在过而悲伤流泪！ 人类的形象变得比正午的太阳还要夺目，放射出的光芒令哈利的皮肤感到一阵温暖。哈利向死亡的阴影倾泄出了他全部的蔑视，打开了心中所有的闸门，让这个明亮的形象变得越来越耀眼，越来越耀眼。 你不是不可战胜的，总有一天人类会终结你。 如果我能，我会终结你，凭着智慧和魔法和科学的力量。 我不会在死亡的恐惧下颤抖，只要我还有胜利的机会。 我不会任凭死亡碰到我，不会任凭死亡碰到我爱的人们。 即使你真的在我终结你之前终结了我， 也会有人代替我的位置，随后又会有人代替他们的位置， 直到世界的伤口最终被治愈…… 面对死亡的不屈是哈利和伏地魔共有的特点。然而同理心的缺乏使得伏地魔只想逃避他自己的死亡，而哈利却想要为全人类终结死亡。对我来说，这段话就如同《永恒的终结》结尾，哈兰和诺羽决定终结永恒时空，为人类开启走向银河帝国的可能时\n诺羽说道，“这就是地球。这里不仅仅属于永恒组员所有，而是全体人类唯一的故乡，但也是未来无穷冒险的唯一起点。你的任务，就是作出决定。这是你要作出的决定。你和我以及洞穴里头的物品，都会受到物理时间力场的保护，不会受到这次变革的影响。库柏将和那则广告一同消失；永恒时空和我的世纪，也都将一起终结。但我们会留下来，我们两人拥有孩子和孙子，而且人类总有一天能够在群星之间进行太空旅行。” 他转头看着她，而她则回以微笑。这是他一向熟悉 …","date":1582377969,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582377969,"objectID":"657836524c8594804c63fc331607e549","permalink":"https://zxdu.xyz/post/report-hpmor/","publishdate":"2020-02-22T21:26:09+08:00","relpermalink":"/post/report-hpmor/","section":"post","summary":"读书感想","tags":[],"title":"读书感想：《哈利波特与理性之道》","type":"post"},{"authors":["Zhengxiao Du","Jie Tang","Yuhui Ding"],"categories":[],"content":"","date":1573776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1573776000,"objectID":"0bf068cad51390a710162609fe82a1fb","permalink":"https://zxdu.xyz/publication/tkde-19/","publishdate":"2019-11-15T00:00:00Z","relpermalink":"/publication/tkde-19/","section":"publication","summary":"We study the problem of personalized article recommendation, in particular when the user’s preference data is missing or limited, which is knowns as the user cold-start issue in recommender systems. We propose POLAR++, an active recommendation framework that utilizes Bayesian neural networks to capture the uncertainty of user preference, actively selects articles to query the user for feedback, and adaptively learns user preference with one-shot learning. For the article recommendation, we design an attention-based CNN to quantify the similarity between user preference and recommended articles, which signiﬁcantly improves the performance with only a few articles rated by the users. We evaluate the proposed POLAR++ on datasets of different scale and sources. Experimental results demonstrate the effectiveness of the proposed model. We have successfully deployed POLAR++ into AMiner as the recommendation engine for article recommendation, which further conﬁrms the effectiveness of the proposed model.","tags":[],"title":"POLAR++: Active One-shot Personalized Article Recommendation","type":"publication"},{"authors":["Zhengxiao Du","Xiaowei Wang","Hongxia Yang","Jingren Zhou","Jie Tang"],"categories":null,"content":"","date":1563580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1563580800,"objectID":"03a2041179dddcfb4791a622e94f5eac","permalink":"https://zxdu.xyz/publication/kdd-19/","publishdate":"2019-07-20T13:59:10.997616Z","relpermalink":"/publication/kdd-19/","section":"publication","summary":"Cold-start problems are long-standing challenges for practical recommendations. Most existing recommendation algorithms rely on extensive observed data and are brittle to recommendation scenarios with few interactions. This paper addresses such problems using few-shot learning and meta learning. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. To accomplish this, we combine the scenario-specific learning with a model-agnostic sequential meta-learning and unify them into an integrated end-toend framework, namely Scenario-specific Sequential Meta learner (or $s^2$ Meta ). By doing so, our meta-learner produces a generic initial model through aggregating contextual information from a variety of prediction tasks while effectively adapting to specific tasks by leveraging learning-to-learn knowledge. Extensive experiments on various real-world datasets demonstrate that our proposed model can achieve significant gains over the state-of-the-arts for cold-start problems in online recommendation. Deployment is at the Guess You Like session, the front page of the Mobile Taobao.","tags":null,"title":"Sequential Scenario-Specific Meta Learner for Online Recommendation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"1f4cb24553577f6808a73065326a8b9d","permalink":"https://zxdu.xyz/blog/","publishdate":"2019-01-01T00:00:00Z","relpermalink":"/blog/","section":"","summary":"Zhengxiao's Blog","tags":null,"title":"Blog","type":"widget_page"},{"authors":["Yifeng Zhao","Jie Tang","Zhengxiao Du"],"categories":null,"content":"","date":1546300800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1546300800,"objectID":"0289d880c699e53b73f74f6d184dc899","permalink":"https://zxdu.xyz/publication/pakdd-19/","publishdate":"2019-07-20T13:59:10.996845Z","relpermalink":"/publication/pakdd-19/","section":"publication","summary":"","tags":null,"title":"EFCNN: A Restricted Convolutional Neural Network for Expert Finding","type":"publication"},{"authors":["Zhengxiao Du","Jie Tang","Yuhui Ding"],"categories":null,"content":"","date":1536537600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1536537600,"objectID":"b4e5b8386cfa4503b44040458c1c82fc","permalink":"https://zxdu.xyz/publication/pkdd-18/","publishdate":"2019-07-20T13:59:10.995108Z","relpermalink":"/publication/pkdd-18/","section":"publication","summary":"In this paper, we propose POLAR, an attention-based CNN combined with one-shot learning for personalized article recommendation. Given a query, POLAR uses an attention-based CNN to estimate the relevance score between the query and related articles. The attention mechanism can help significantly improve the relevance estimation. For example, on AMiner, this can help achieve a +5.0% improvement in terms of NDCG@3. One more challenge in personalized article recommendation is how to collect statistically sufficient training data for a recommendation model. POLAR combines a one-shot learning function into the recommendation model, which further gains significant improvements. For example, on AMiner, with only 1.6 feedbacks on average, POLAR achieves 2.7% improvement by NDCG@3. We evaluate the proposed POLAR on three different datasets: AMiner, Patent, and RARD. Experimental results demonstrate the effectiveness of the proposed model. Recently, we have successfully deployed POLAR into AMiner as the recommendation engine for article recommendation, which further confirms the effectiveness of the proposed model.","tags":null,"title":"POLAR: Attention-Based CNN for One-Shot Personalized Article Recommendation","type":"publication"}]