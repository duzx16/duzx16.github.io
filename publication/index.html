<!DOCTYPE html>
<!-- This site was created with Wowchemy. https://www.wowchemy.com -->
<!-- Last Published: January 31, 2025 --><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.9.0 for Hugo" />
  

  
  












  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.047268c6dd09ad74ba54a0ba71837064.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1.9.4/css/academicons.min.css" integrity="sha512-IW0nhlW5MgNydsXJO40En2EoCkTTjZhI3yuODrZIc8cQ4h1XcF53PsqDHa09NqnkXuIe0Oiyyj171BqZFwISBw==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.7eaca94f0cfe2f9115699dbdb8fbc775.css" />

  
  
  

  
  
  
  
  
  
  
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" >
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" disabled>
  

  
  



























  
  
  






  <meta name="author" content="Zhengxiao Du" />





  

<meta name="description" content="Personal homepage for Zhengxiao Du, a CS PhD student at Tsinghua University" />



<link rel="alternate" hreflang="en-us" href="https://zxdu.xyz/publication/" />
<link rel="canonical" href="https://zxdu.xyz/publication/" />



  <link rel="manifest" href="/manifest.webmanifest" />



<link rel="icon" type="image/png" href="/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_32x32_fill_lanczos_center_3.png" />
<link rel="apple-touch-icon" type="image/png" href="/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_180x180_fill_lanczos_center_3.png" />

<meta name="theme-color" content="#1565c0" />










  
  






<meta property="twitter:card" content="summary" />

  <meta property="twitter:site" content="@wowchemy" />
  <meta property="twitter:creator" content="@wowchemy" />
<meta property="twitter:image" content="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_512x512_fill_lanczos_center_3.png" />



  

<meta property="og:type" content="website" />
<meta property="og:site_name" content="Zhengxiao Du" />
<meta property="og:url" content="https://zxdu.xyz/publication/" />
<meta property="og:title" content="Publications | Zhengxiao Du" />
<meta property="og:description" content="Personal homepage for Zhengxiao Du, a CS PhD student at Tsinghua University" /><meta property="og:image" content="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />

  
    <meta property="og:updated_time" content="2024-12-03T00:00:00&#43;00:00" />
  










  
  
  

  
  
    <link rel="alternate" href="/publication/index.xml" type="application/rss+xml" title="Zhengxiao Du" />
  

  


  
  <title>Publications | Zhengxiao Du</title>

  
  
  
  











</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="3a079e7dad19be978a318345a7749d34" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.e6724bea461adf2acc4820665a329eca.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header header--fixed">
  
  
  
  
  












<header>
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Zhengxiao Du</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Zhengxiao Du</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#about"><span>Home</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#experience"><span>Experience</span></a>
          </li>

          
          

          

          
          
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/blog"><span>Blog</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#publications"><span>Publications</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
          

          <li class="nav-item">
            <a class="nav-link " href="/files/CV.pdf"><span>CV</span></a>
          </li>

          
          

          

          
          
          
            
          

          

          
          
          
          

          
            
              
              
            
            
              
              
              
                
              
              
            
          

          <li class="nav-item">
            <a class="nav-link " href="/#contact"><span>Contact</span></a>
          </li>

          
          

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        
          
        

        
        
        
        <li class="nav-item">
          <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
        </li>
        

        
        
        
        <li class="nav-item dropdown theme-dropdown">
          <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
            <i class="fas fa-moon" aria-hidden="true"></i>
          </a>
          <div class="dropdown-menu">
            <a href="#" class="dropdown-item js-set-theme-light">
              <span>Light</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-dark">
              <span>Dark</span>
            </a>
            <a href="#" class="dropdown-item js-set-theme-auto">
              <span>Automatic</span>
            </a>
          </div>
        </li>
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    
















  

  
  
  
    
  
<div class="universal-wrapper pt-3">
  <h1>Publications</h1>

  

  
</div>



<div class="universal-wrapper">
  <div class="row">
    <div class="col-lg-12">

      

      
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      
        
        
      

      <div class="form-row mb-4">
        <div class="col-auto">
          <input type="search" class="filter-search form-control form-control-sm" placeholder="Search..." autocapitalize="off"
          autocomplete="off" autocorrect="off" role="textbox" spellcheck="false">
        </div>
        <div class="col-auto">
          <select class="pub-filters pubtype-select form-control form-control-sm" data-filter-group="pubtype">
            <option value="*">Type</option>
            
            <option value=".pubtype-1">
              1
            </option>
            
            <option value=".pubtype-2">
              2
            </option>
            
            <option value=".pubtype-3">
              3
            </option>
            
          </select>
        </div>
        <div class="col-auto">
          <select class="pub-filters form-control form-control-sm" data-filter-group="year">
            <option value="*">Date</option>
            
            
            
            <option value=".year-2024">
              2024
            </option>
            
            <option value=".year-2022">
              2022
            </option>
            
            <option value=".year-2021">
              2021
            </option>
            
            <option value=".year-2019">
              2019
            </option>
            
            <option value=".year-2018">
              2018
            </option>
            
            
          </select>
        </div>
      </div>

      <div id="container-publications">
        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-3 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/aohan-zeng/">Aohan Zeng</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/zhengxiao-du/">Zhengxiao Du</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/mingdao-liu/">Mingdao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/kedong-wang/">Kedong Wang</a></span>, <span >
      <a href="/authors/shengmin-jiang/">Shengmin Jiang</a></span>, <span >
      <a href="/authors/lei-zhao/">Lei Zhao</a></span>, <span >
      <a href="/authors/yuxiao-dong/">Yuxiao Dong</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    December, 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Preprint
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/glm-4-voice/" >GLM-4-Voice: Towards Intelligent and Human-Like End-to-End Spoken Chatbot</a>
  </div>

  
  <a href="/publication/glm-4-voice/"  class="summary-link">
    <div class="article-style">
      <p>We introduce GLM-4-Voice, an intelligent and human-like end-to-end spoken chatbot. It supports both Chinese and English, engages in real-time voice conversations, and varies vocal nuances such as emotion, intonation, speech rate, and dialect according to user instructions. GLM-4-Voice uses an ultra-low bitrate (175bps), single-codebook speech tokenizer with 12.5Hz frame rate derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. To efficiently transfer knowledge from text to speech modalities, we synthesize speech-text interleaved data from existing text pre-training corpora using a text-to-token model. We continue pre-training from the pre-trained text language model GLM-4-9B with a combination of unsupervised speech data, interleaved speech-text data, and supervised speech-text data, scaling up to 1 trillion tokens, achieving state-of-the-art performance in both speech language modeling and spoken question answering. We then fine-tune the pre-trained model with high-quality conversational speech data, achieving superior performance compared to existing baselines in both conversational ability and speech quality. The open models can be accessed through <a href="https://github.com/THUDM/GLM-4-Voice" target="_blank" rel="noopener">https://github.com/THUDM/GLM-4-Voice</a> and <a href="https://huggingface.co/THUDM/glm-4-voice-9b" target="_blank" rel="noopener">https://huggingface.co/THUDM/glm-4-voice-9b</a>.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2411.17607" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/aohan-zeng/">Aohan Zeng</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/zhengxiao-du/">Zhengxiao Du</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/mingdao-liu/">Mingdao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/lei-zhang/">Lei Zhang</a></span>, <span >
      <a href="/authors/shengmin-jiang/">Shengmin Jiang</a></span>, <span >
      <a href="/authors/yuxiao-dong/">Yuxiao Dong</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November, 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      ICLR 2025
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/iclr-25/" >Scaling Speech-Text Pre-training with Synthetic Interleaved Data</a>
  </div>

  
  <a href="/publication/iclr-25/"  class="summary-link">
    <div class="article-style">
      <p>Speech language models (SpeechLMs) accept speech input and produce speech output, allowing for more natural human-computer interaction compared to text-based large language models (LLMs). Traditional approaches for developing SpeechLMs are constrained by the limited availability of unsupervised speech data and parallel speech-text data, which are significantly less abundant than text pre-training data, thereby limiting their scalability as LLMs. We propose a novel approach to scaling speech-text pre-training by leveraging large-scale synthetic interleaved data derived from text corpora, eliminating the need for parallel speech-text datasets. Our method efficiently constructs speech-text interleaved data by sampling text spans from existing text corpora and synthesizing corresponding speech spans using a text-to-token model, bypassing the need to generate actual speech. We also employ a supervised speech tokenizer derived from an automatic speech recognition (ASR) model by incorporating a vector-quantized bottleneck into the encoder. This supervised training approach results in discrete speech tokens with strong semantic preservation even at lower frame rates (e.g. 12.5Hz), while still maintaining speech reconstruction quality. Starting from a pre-trained language model and scaling our pre-training to 1 trillion tokens (with 600B synthetic interleaved speech-text data), we achieve state-of-the-art performance in speech language modeling and spoken question answering, improving performance on spoken questions tasks from the previous SOTA of 13% (Moshi) to 31%. We further demonstrate that by fine-tuning the pre-trained model with speech dialogue data, we can develop an end-to-end spoken chatbot that achieves competitive performance comparable to existing baselines in both conversational abilities and speech quality, even operating exclusively in the speech domain.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2411.17607" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2024">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/aohan-zeng/">Aohan Zeng</a></span>, <span >
      <a href="/authors/yuxiao-dong/">Yuxiao Dong</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March, 2024
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <strong>NeurIPS 2024</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/neurips-24/" >Understanding Emergent Abilities of Language Models from the Loss Perspective</a>
  </div>

  
  <a href="/publication/neurips-24/"  class="summary-link">
    <div class="article-style">
      <p>Recent studies have put into question the belief that emergent abilities in language models are exclusive to large models. This skepticism arises from two observations: 1) smaller models can also exhibit high performance on emergent abilities and 2) there is doubt on the discontinuous metrics used to measure these abilities. In this paper, we propose to study emergent abilities in the lens of pre-training loss, instead of model size or training compute. We demonstrate that the models with the same pre-training loss, but different model and data sizes, generate the same performance on various downstream tasks. We also discover that a model exhibits emergent abilities on certain tasks &ndash; regardless of the continuity of metrics &ndash; when its pre-training loss falls below a specific threshold. Before reaching this threshold, its performance remains at the level of random guessing. This inspires us to redefine emergent abilities as those that manifest in models with lower pre-training losses, highlighting that these abilities cannot be predicted by merely extrapolating the performance trends of models with higher pre-training losses.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2403.15796" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/aohan-zeng/">Aohan Zeng</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/xiao-liu/">Xiao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/zihan-wang/">Zihan Wang</a></span>, <span >
      <a href="/authors/hanyu-lai/">Hanyu Lai</a></span>, <span >
      <a href="/authors/ming-ding/">Ming Ding</a></span>, <span >
      <a href="/authors/zhuoyi-yang/">Zhuoyi Yang</a></span>, <span >
      <a href="/authors/yifan-xu/">Yifan Xu</a></span>, <span >
      <a href="/authors/wendi-zheng/">Wendi Zheng</a></span>, <span >
      <a href="/authors/xiao-xia/">Xiao Xia</a></span>, <span >
      <a href="/authors/weng-lam-tam/">Weng Lam Tam</a></span>, <span >
      <a href="/authors/zixuan-ma/">Zixuan Ma</a></span>, <span >
      <a href="/authors/yufei-xue/">Yufei Xue</a></span>, <span >
      <a href="/authors/jidong-zhai/">Jidong Zhai</a></span>, <span >
      <a href="/authors/wenguang-chen/">Wenguang Chen</a></span>, <span >
      <a href="/authors/peng-zhang/">Peng Zhang</a></span>, <span >
      <a href="/authors/yuxiao-dong/">Yuxiao Dong</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    October, 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <strong>ICLR 2023</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/glm-130b/" >GLM-130B: An Open Bilingual Pre-Trained Model</a>
  </div>

  
  <a href="/publication/glm-130b/"  class="summary-link">
    <div class="article-style">
      <p>We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B &ndash; the largest Chinese language model &ndash; across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4×RTX 3090 (24G) or 8×RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at <a href="https://github.com/THUDM/GLM-130B" target="_blank" rel="noopener">https://github.com/THUDM/GLM-130B</a></p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/THUDM/GLM-130B" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2210.02414" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/xiao-liu/">Xiao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/kaixuan-ji/">Kaixuan Ji</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/yicheng-fu/">Yicheng Fu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/weng-lam-tam/">Weng Lam Tam</a></span>, <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/zhilin-yang/">Zhilin Yang</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March, 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      <strong>ACL 2022</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/p-tuning-v2/" >P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</a>
  </div>

  
  <a href="/publication/p-tuning-v2/"  class="summary-link">
    <div class="article-style">
      <p>Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/THUDM/P-tuning-v2" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2110.07602" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2022.acl-short.8" target="_blank" rel="noopener">
    <i class="ai ai-acm mr-1"></i>Paper</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2022">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/yujie-qian/">Yujie Qian</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/xiao-liu/">Xiao Liu</a></span>, <span >
      <a href="/authors/ming-ding/">Ming Ding</a></span>, <span >
      <a href="/authors/jiezhong-qiu/">Jiezhong Qiu</a></span>, <span >
      <a href="/authors/zhilin-yang/">Zhilin Yang</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March, 2022
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      the 60th Annual Meeting of the Association for Computational Linguistics, <strong>ACL 2022</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/glm-21/" >GLM: General Language Model Pretraining with Autoregressive Blank Infilling</a>
  </div>

  
  <a href="/publication/glm-21/"  class="summary-link">
    <div class="article-style">
      <p>There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). On the other hand, NLP tasks differ in nature, with three main categories being natural language understanding (NLU), unconditional generation, and conditional generation, while none of the pretraining frameworks performs the best for all tasks. We propose a General Language Model (GLM)  based on autoregressive blank infilling to address this challenge. The proposed architecture has two major benefits: (1) It improves pretrain-finetune consistency via cloze-style finetuning and naturally handles variable-length blank infilling which is crucial for many downstream tasks. Empirically, GLM substantially outperforms BERT on the SuperGLUE natural language understanding benchmark with the same amount of pretraining data and steps. (2) It is flexible enough to handle various NLP tasks with a single pretrained model. GLM with 1.25x parameters of BERT-Large achieves the best performance in NLU, conditional and unconditional generation at the same time, demonstrating its generalizability to different downstream tasks.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/THUDM/GLM" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2103.10360" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://aclanthology.org/2022.acl-long.26" target="_blank" rel="noopener">
    <i class="ai ai-acm mr-1"></i>Paper</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/chang-zhou/">Chang Zhou</a></span>, <span >
      <a href="/authors/jiangchao-yao/">Jiangchao Yao</a></span>, <span >
      <a href="/authors/teng-tu/">Teng Tu</a></span>, <span >
      <a href="/authors/letian-cheng/">Letian Cheng</a></span>, <span >
      <a href="/authors/hongxia-yang/">Hongxia Yang</a></span>, <span >
      <a href="/authors/jingren-zhou/">Jingren Zhou</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    August, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Transactions on Knowledge and Data Engineering, <strong>TKDE</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/cogkr-19/" >CogKR: Cognitive Graph for Multi-hop Knowledge Reasoning</a>
  </div>

  
  <a href="/publication/cogkr-19/"  class="summary-link">
    <div class="article-style">
      <p>Inferring new facts from an existing knowledge graph with explainable reasoning processes is an important problem, known as knowledge graph (KG) reasoning. The problem is often formulated as finding the specific path that represents the query relation and connects the query entity and the correct answer. However, due to the limited expressiveness of individual paths, the majority of previous works failed to capture the complex subgraph structure in the graph. We propose CogKR that traverses the knowledge graph to conduct multi-hop reasoning. More specifically, motivated by the dual process theory from cognitive science, our framework is composed of an extension module and a reasoning module. By setting up a cognitive graph through iteratively coordinating the two modules, CogKR can cope with more complex reasoning scenarios in the form of subgraphs instead of individual paths. Experiments on three knowledge graph reasoning benchmarks demonstrate that CogKR achieves significant improvements in accuracy compared with previous methods while providing the explainable capacity. Moreover, we evaluate CogKR on the challenging one-shot link prediction task, exhibiting the superiority of the framework on accuracy and scalability compared to the state-of-the-art approaches.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/THUDM/CogKR" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1906.05489" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/9512424" target="_blank" rel="noopener">
    <i class="ai ai-ieee mr-1"></i>Paper</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/himank-yadav/">Himank Yadav</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/thorsten-joachims/">Thorsten Joachims</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      44th International <strong>ACM SIGIR</strong> Conference on Research and Development in Information RetrievalJuly 2021, <strong>SIGIR 2021</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/fair-19/" >Policy-Gradient Training of Fair and Unbiased Ranking Functions</a>
  </div>

  
  <a href="/publication/fair-19/"  class="summary-link">
    <div class="article-style">
      <p>While implicit feedback (e.g., clicks, dwell times, etc.) is an abundant and attractive source of data for learning to rank, it can produce unfair ranking policies for both exogenous and endogenous reasons. Exogenous reasons typically manifest themselves as biases in the training data, which then get reflected in the learned ranking policy and often lead to rich-get-richer dynamics. Moreover, even after the correction of such biases, reasons endogenous to the design of the learning algorithm can still lead to ranking policies that do not allocate exposure among items in a fair way. To address both exogenous and endogenous sources of unfairness, we present the first learning-to-rank approach that addresses both presentation bias and merit-based fairness of exposure simultaneously. Specifically, we define a class of amortized fairness-of-exposure constraints that can be chosen based on the needs of an application, and we show how these fairness criteria can be enforced despite the selection biases in implicit feedback data. The key result is an efficient and flexible policy-gradient algorithm, called FULTR, which is the first to enable the use of counterfactual estimators for both utility estimation and fairness constraints. Beyond the theoretical justification of the framework, we show empirically that the proposed algorithm can learn accurate and fair ranking policies from biased and noisy feedback.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/him229/fultr" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1911.08054" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://dl.acm.org/doi/abs/10.1145/3404835.3462953" target="_blank" rel="noopener">
    <i class="ai ai-acmdl mr-1"></i>Paper</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2021">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="/authors/xiao-liu/">Xiao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="/authors/yanan-zheng/">Yanan Zheng</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/ming-ding/">Ming Ding</a></span>, <span >
      <a href="/authors/jiezhong-qiu/">Jiezhong Qiu</a></span>, <span >
      <a href="/authors/zhilin-yang/">Zhilin Yang</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March, 2021
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      AI Open
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/p-tuning/" >GPT Understands, Too</a>
  </div>

  
  <a href="/publication/p-tuning/"  class="summary-link">
    <div class="article-style">
      <p>While GPTs with traditional ﬁne-tuning fail to achieve strong results on natural language understanding (NLU), we show that GPTs can be better than or comparable to similar-sized BERTs on NLU tasks with a novel method P-tuningwhich employs trainable continuous prompt embeddings. On the knowledge probing (LAMA) benchmark, the best GPT recovers 64% (P@1) of world knowledge without any additional text provided during test time, which substantially improves the previous best by 20+ percentage points. On the SuperGlue benchmark, GPTs achieve comparable and sometimes better performance to similar-sized BERTs in supervised learning. Importantly, we ﬁnd that P-tuning also improves BERTs’ performance in both few-shot and supervised settings while largely reducing the need for prompt engineering. Consequently, Ptuning outperforms the state-of-the-art approaches on the few-shot SuperGlue benchmark.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/2103.10385" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/thudm/p-tuning" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-2 year-2019">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>, <span >
      <a href="/authors/yuhui-ding/">Yuhui Ding</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    November, 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      IEEE Transactions on Knowledge and Data Engineering, <strong>TKDE</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/tkde-19/" >POLAR&#43;&#43;: Active One-shot Personalized Article Recommendation</a>
  </div>

  
  <a href="/publication/tkde-19/"  class="summary-link">
    <div class="article-style">
      <p>We study the problem of personalized article recommendation, in particular when the user’s preference data is missing or limited, which is knowns as the user cold-start issue in recommender systems. We propose POLAR++, an active recommendation framework that utilizes Bayesian neural networks to capture the uncertainty of user preference, actively selects articles to query the user for feedback, and adaptively learns user preference with one-shot learning. For the article recommendation, we design an attention-based CNN to quantify the similarity between user preference and recommended articles, which signiﬁcantly improves the performance with only a few articles rated by the users. We evaluate the proposed POLAR++ on datasets of different scale and sources. Experimental results demonstrate the effectiveness of the proposed model. We have successfully deployed POLAR++ into AMiner as the recommendation engine for article recommendation, which further conﬁrms the effectiveness of the proposed model.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://ieeexplore.ieee.org/document/8901999" target="_blank" rel="noopener">
    <i class="ai ai-ieee mr-1"></i>Paper</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2019">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/xiaowei-wang/">Xiaowei Wang</a></span>, <span >
      <a href="/authors/hongxia-yang/">Hongxia Yang</a></span>, <span >
      <a href="/authors/jingren-zhou/">Jingren Zhou</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    July, 2019
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      25th <em>ACM SIGKDD</em> International Conference on Knowledge Discovery &amp; Data Mining, <strong>KDD 2019</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/kdd-19/" >Sequential Scenario-Specific Meta Learner for Online Recommendation</a>
  </div>

  
  <a href="/publication/kdd-19/"  class="summary-link">
    <div class="article-style">
      <p>Cold-start problems are long-standing challenges for practical recommendations. Most existing recommendation algorithms rely on extensive observed data and are brittle to recommendation scenarios with few interactions. This paper addresses such problems using few-shot learning and meta learning. Our approach is based on the insight that having a good generalization from a few examples relies on both a generic model initialization and an effective strategy for adapting this model to newly arising tasks. To accomplish this, we combine the scenario-specific learning with a model-agnostic sequential meta-learning and unify them into an integrated end-toend framework, namely Scenario-specific Sequential Meta learner (or $s^2$ Meta ). By doing so, our meta-learner produces a generic initial model through aggregating contextual information from a variety of prediction tasks while effectively adapting to specific tasks by leveraging learning-to-learn knowledge. Extensive experiments on various real-world datasets demonstrate that our proposed model can achieve significant gains over the state-of-the-arts for cold-start problems in online recommendation. Deployment is at the Guess You Like session, the front page of the Mobile Taobao.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://github.com/THUDM/ScenarioMeta" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header btn-sm" href="https://arxiv.org/abs/1906.00391" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>


  </div>
  

</div>

        </div>

        

        
          
        

        <div class="grid-sizer col-lg-12 isotope-item pubtype-1 year-2018">
          











  


<div class="card-simple view-card">

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span class="author-highlighted">
      <a href="/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="/authors/jie-tang/">Jie Tang</a></span>, <span >
      <a href="/authors/yuhui-ding/">Yuhui Ding</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    September, 2018
  </span>
  

  
  <span class="middot-divider"></span>
  <span class="pub-publication">
    
      Machine Learning and Knowledge Discovery in Databases - European Conference, <strong>ECML/PKDD 2018</strong>
    
  </span>
  

  

  
  
  
  

  
  

</div>

  

  
  
  

  <div class="section-subheading article-title mb-1 mt-3">
    <a href="/publication/pkdd-18/" >POLAR: Attention-Based CNN for One-Shot Personalized Article Recommendation</a>
  </div>

  
  <a href="/publication/pkdd-18/"  class="summary-link">
    <div class="article-style">
      <p>In this paper, we propose POLAR, an attention-based CNN combined with one-shot learning for personalized article recommendation. Given a query, POLAR uses an attention-based CNN to estimate the relevance score between the query and related articles. The attention mechanism can help significantly improve the relevance estimation. For example, on AMiner, this can help achieve a +5.0% improvement in terms of NDCG@3. One more challenge in personalized article recommendation is how to collect statistically sufficient training data for a recommendation model. POLAR combines a one-shot learning function into the recommendation model, which further gains significant improvements. For example, on AMiner, with only 1.6 feedbacks on average, POLAR achieves 2.7% improvement by NDCG@3. We evaluate the proposed POLAR on three different datasets: AMiner, Patent, and RARD. Experimental results demonstrate the effectiveness of the proposed model. Recently, we have successfully deployed POLAR into AMiner as the recommendation engine for article recommendation, which further confirms the effectiveness of the proposed model.</p>
    </div>
  </a>
  

  
  <div class="btn-links">
    








  



<a class="btn btn-outline-primary btn-page-header btn-sm" href="/publication/pkdd-18/pkdd-18.pdf" target="_blank" rel="noopener">
  PDF
</a>

















  </div>
  

</div>

        </div>

        
      </div>

    </div>
  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  












  
  
  
  
  













  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>





  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  


<script src="/js/vendor-bundle.min.938a3a7554cd9f6602290411f64d2617.js"></script>




  
    <script src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
  

  
  

  













  
  <script id="search-hit-fuse-template" type="text/x-template">
    <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
    </div>
  </script>
  
    <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
  












  
  
  
  
  
  
  

















<script id="page-data" type="application/json">{"use_headroom":true}</script>


  <script src="/js/wowchemy-headroom.db4755770454eb63685f8de785c0a172.js" type="module"></script>









  
  


<script src="/en/js/wowchemy.min.5434e222df551c9e1b69d4b459b17aeb.js"></script>







  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        
        <pre><code></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>


  <script src="/js/wowchemy-publication.9137013a66774049159934c29c3f0205.js" type="module"></script>


















</body>
</html>
