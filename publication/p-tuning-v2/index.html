<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  
  









  




  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Zhengxiao Du" />

  
  
  
    
  
  <meta name="description" content="Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research." />

  
  <link rel="alternate" hreflang="en-us" href="https://zxdu.xyz/publication/p-tuning-v2/" />

  
  
  
    <meta name="theme-color" content="#1565c0" />
  

  
  
    
    <script src="https://zxdu.xyz/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="https://zxdu.xyz/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha512-W0xM4mr6dEP9nREo7Z9z+9X70wytKvMGeDsj7ps2+xg5QPrEBXC8tAW1IFnzjR6eoJ90JmCnFzerQJTLzIEHjA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="https://zxdu.xyz/css/wowchemy.7fa18a541b489204d14a3f8fd75bb3b5.css" />

  



  

  

  




  
  
  

  

  

  <link rel="icon" type="image/png" href="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://zxdu.xyz/publication/p-tuning-v2/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Zhengxiao Du" />
  <meta property="og:url" content="https://zxdu.xyz/publication/p-tuning-v2/" />
  <meta property="og:title" content="P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks | Zhengxiao Du" />
  <meta property="og:description" content="Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research." /><meta property="og:image" content="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-03-15T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-03-15T00:00:00&#43;00:00">
  

  


    









<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "Article",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://zxdu.xyz/publication/p-tuning-v2/"
  },
  "headline": "P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks",
  
  "datePublished": "2022-03-15T00:00:00Z",
  "dateModified": "2022-03-15T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Xiao Liu"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Zhengxiao Du",
    "logo": {
      "@type": "ImageObject",
      "url": "https://zxdu.xyz/media/icon_hu201e1a72013825f24e615b24fa4a82e9_325334_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research."
}
</script>

  

  

  

  





  <title>P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks | Zhengxiao Du</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="60431ca2d54a82f8ec6c5a10313bc89f" >

  
  
  
  
  
  
  
  
  
  <script src="https://zxdu.xyz/js/wowchemy-init.min.6619cc29fe1a874b6ea1a0359aab5cfb.js"></script>

  




  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="https://zxdu.xyz/">Zhengxiao Du</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="https://zxdu.xyz/">Zhengxiao Du</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://zxdu.xyz/#about"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://zxdu.xyz/#experience"><span>Experience</span></a>
        </li>

        
        

        

        
        
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://zxdu.xyz/blog"><span>Blog</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://zxdu.xyz/#publications"><span>Publications</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="https://zxdu.xyz/files/CV.pdf"><span>CV</span></a>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
      

      
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    








<div class="pub">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>P-Tuning: Prompt Tuning Can Be Comparable to Fine-tuning Across Scales and Tasks</h1>

  

  
    


<div class="article-metadata">

  
  
  
  
  <div>
    

  <span >
      <a href="https://zxdu.xyz/authors/xiao-liu/">Xiao Liu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="https://zxdu.xyz/authors/kaixuan-ji/">Kaixuan Ji</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="https://zxdu.xyz/authors/yicheng-fu/">Yicheng Fu</a></span><i class="author-notes fas fa-info-circle" data-toggle="tooltip" title="Equal contribution"></i>, <span >
      <a href="https://zxdu.xyz/authors/weng-lam-tam/">Weng Lam Tam</a></span>, <span class="author-highlighted">
      <a href="https://zxdu.xyz/authors/admin/">Zhengxiao Du</a></span>, <span >
      <a href="https://zxdu.xyz/authors/zhilin-yang/">Zhilin Yang</a></span>, <span >
      <a href="https://zxdu.xyz/authors/jie-tang/">Jie Tang</a></span>
  </div>
  
  

  
  <span class="article-date">
    
    
      
    
    March 2022
  </span>
  

  

  

  
  
  
  
  
  

  
  

</div>

    




<div class="btn-links mb-3">
  
  








  


















  
  
  
    
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://github.com/THUDM/P-tuning-v2" target="_blank" rel="noopener">
    <i class="fab fa-github mr-1"></i>Code</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://arxiv.org/abs/2110.07602" target="_blank" rel="noopener">
    <i class="ai ai-arxiv mr-1"></i>Preprint</a>

  
  
  
  
  
  
  
    
  
  <a class="btn btn-outline-primary btn-page-header" href="https://aclanthology.org/2022.acl-short.8" target="_blank" rel="noopener">
    <i class="ai ai-acm mr-1"></i>Paper</a>


</div>


  
</div>



  <div class="article-container">

    
    <h3>Abstract</h3>
    <p class="pub-abstract">Prompt tuning, which only tunes continuous prompts with a frozen language model, substantially reduces per-task storage and memory usage at training. However, in the context of NLU, prior work reveals that prompt tuning does not perform well for normal-sized pretrained models. We also find that existing methods of prompt tuning cannot handle hard sequence labeling tasks, indicating a lack of universality. We present a novel empirical finding that properly optimized prompt tuning can be universally effective across a wide range of model scales and NLU tasks. It matches the performance of finetuning while having only 0.1%-3% tuned parameters. Our method P-Tuning v2 is an implementation of Deep Prompt Tuning (CITATION) optimized and adapted for NLU. Given the universality and simplicity of P-Tuning v2, we believe it can serve as an alternative to finetuning and a strong baseline for future research.</p>
    

    
    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Type</div>
          <div class="col-12 col-md-9">
            <a href="https://zxdu.xyz/publication/#2">
              Journal article
            </a>
          </div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    
    <div class="row">
      <div class="col-md-1"></div>
      <div class="col-md-10">
        <div class="row">
          <div class="col-12 col-md-3 pub-row-heading">Publication</div>
          <div class="col-12 col-md-9"><strong>ACL 2022</strong></div>
        </div>
      </div>
      <div class="col-md-1"></div>
    </div>
    <div class="d-md-none space-below"></div>
    

    <div class="space-below"></div>

    <div class="article-style"></div>

    


















  
  
    




  
    




  
    




  
    




  
    



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://zxdu.xyz/"><img class="avatar mr-3 avatar-circle" src="https://zxdu.xyz/authors/admin/avatar_hu7e2cbb0813f9c409aba9226cd1386a71_4073407_270x270_fill_lanczos_center_3.png" alt="Zhengxiao Du"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://zxdu.xyz/">Zhengxiao Du</a></h5>
      <h6 class="card-subtitle">PhD Student</h6>
      <p class="card-text">My research interests include machine learning and data mining.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.co.uk/citations?user=A8x07E0AAAAJ" target="_blank" rel="noopener">
        <i class="ai ai-google-scholar"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/duzx16" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
    
    
    
    
      
    
    <li>
      <a href="https://zxdu.xyz/files/CV.pdf" >
        <i class="ai ai-cv"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>


  
    




  
    




  
















  </div>
</div>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <!-- raw HTML omitted -->Wowchemy<!-- raw HTML omitted --> — the free, <!-- raw HTML omitted -->open source<!-- raw HTML omitted --> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="https://zxdu.xyz/js/vendor-bundle.min.d26cbef546776b4d6032d1ea3dafadab.js"></script>

    
    
    
      
      
        <script src="https://cdn.jsdelivr.net/gh/desandro/imagesloaded@v4.1.4/imagesloaded.pkgd.min.js" integrity="sha512-S5PZ9GxJZO16tT9r3WJp/Safn31eu8uWrzglMahDT4dsmgqWonRY9grk3j+3tfuPr9WJNsfooOR7Gi7HL5W2jw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/metafizzy/isotope@v3.0.6/dist/isotope.pkgd.min.js" integrity="sha512-Zq2BOxyhvnRFXu0+WE6ojpZLOU2jdnqbrM1hmVdGzyeCa1DgM3X5Q4A/Is9xA1IkbUeDd7755dNNI/PzSf2Pew==" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/highlight.min.js" integrity="sha512-Ypjm0o7jOxAd4hpdoppSEN0TQOC19UtPAqD+4s5AlXmUvbmmS/YMxYqAqarQYyxTnB6/rqip9qcxlNB/3U9Wdg==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@10.2.1/build/languages/r.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    

    
    

    
    
    

    
    

    
    
    
    

    
    

    
    
    
    
    
    
    
    
    
    <script src="https://zxdu.xyz/en/js/wowchemy.min.82d7c36c07a5c6437b661d52afb75da3.js"></script>

    






</body>
</html>
